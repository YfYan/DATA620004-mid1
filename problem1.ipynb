{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,optimizers,datasets,Sequential\n",
    "import os\n",
    "from resnet import resnet18,resnet34\n",
    "import numpy as np\n",
    "import datetime\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x,y):\n",
    "    x=2 * tf.cast(x,dtype=tf.float32)/255.-1\n",
    "    y=tf.cast(y,dtype=tf.int32)\n",
    "    return x,y\n",
    "\n",
    "(x,y),(x_test,y_test) = datasets.cifar100.load_data()\n",
    "x = x.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "y=tf.squeeze(y,axis=1)\n",
    "y_test=tf.squeeze(y_test,axis=1)\n",
    "\n",
    "train_db=tf.data.Dataset.from_tensor_slices((x,y))\n",
    "train_db=train_db.map(preprocess).batch(50)\n",
    "\n",
    "test_db=tf.data.Dataset.from_tensor_slices((x_test,y_test))\n",
    "test_db=test_db.map(preprocess).batch(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据增强方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutout_mask(img,label):\n",
    "    length = 16\n",
    "    img = img.numpy()\n",
    "    label = label.numpy()\n",
    "  \n",
    "    batch_size,h,w,channel_num = img.shape\n",
    "    for i in range(0,batch_size,2):\n",
    "        y = np.random.randint(h)\n",
    "        x = np.random.randint(w)\n",
    "\n",
    "        y1 = np.clip(y - length // 2, 0, h)\n",
    "        y2 = np.clip(y + length // 2, 0, h)\n",
    "        x1 = np.clip(x - length // 2, 0, w)\n",
    "        x2 = np.clip(x + length // 2, 0, w)\n",
    "        \n",
    "        img[i, y1: y2, x1: x2, :] = 0 \n",
    "    \n",
    "    return img, label\n",
    "\n",
    "def mixup_mask(img,label):\n",
    "    img = img.numpy()\n",
    "    label = label.numpy()\n",
    "    batch_size,h,w,channel_num = img.shape\n",
    "    \n",
    "    for i in range(0,batch_size,2):\n",
    "        mixup_idx = np.random.randint(0,batch_size)\n",
    "        lamda = np.random.uniform()\n",
    "        img[i,:,:,:] = lamda*img[i,:,:,:] + (1-lamda) * img[mixup_idx,:,:,:]\n",
    "        label[i,:] = lamda*label[i,:] + (1-lamda) * label[mixup_idx,:]\n",
    "    return img,label\n",
    "\n",
    "def cutmix_mask(img,label):\n",
    "    length = 16\n",
    "    img = img.numpy()\n",
    "    label = label.numpy()\n",
    "    batch_size,h,w,channel_num = img.shape\n",
    "    for i in range(0,batch_size,2):\n",
    "        mixup_idx = np.random.randint(0,batch_size)\n",
    "        y = np.random.randint(h)\n",
    "        x = np.random.randint(w)\n",
    "\n",
    "        y1 = np.clip(y - length // 2, 0, h)\n",
    "        y2 = np.clip(y + length // 2, 0, h)\n",
    "        x1 = np.clip(x - length // 2, 0, w)\n",
    "        x2 = np.clip(x + length // 2, 0, w)\n",
    "        \n",
    "        lamda = 1 - (y2-y1)*(x2 - x1)/(h*w)\n",
    "        img[i, y1: y2, x1: x2, :] = img[mixup_idx, y1: y2, x1: x2, :]\n",
    "        label[i,:] = lamda*label[i,:] + (1-lamda) * label[mixup_idx,:]\n",
    "    return img,label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet34()\n",
    "model.build(input_shape=(None,32,32,3))\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '_baseline'\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练前清空log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 losses: 4.627544403076172\n",
      "0 100 losses: 4.504234790802002\n",
      "0 200 losses: 4.452385902404785\n",
      "0 300 losses: 4.341122150421143\n",
      "0 400 losses: 4.169934272766113\n",
      "0 500 losses: 3.9839138984680176\n",
      "0 600 losses: 4.03331995010376\n",
      "0 700 losses: 3.9039089679718018\n",
      "0 800 losses: 3.8345959186553955\n",
      "0 900 losses: 3.6105217933654785\n",
      "Epoch 1, Loss: 4.0777130126953125, Accuracy: 6.052009105682373, Test Loss: 3.6715903282165527, Test Accuracy: 12.940006256103516\n",
      "1 0 losses: 3.8005781173706055\n",
      "1 100 losses: 3.794304847717285\n",
      "1 200 losses: 3.796342372894287\n",
      "1 300 losses: 3.6335818767547607\n",
      "1 400 losses: 3.5087995529174805\n",
      "1 500 losses: 3.819816827774048\n",
      "1 600 losses: 3.5144262313842773\n",
      "1 700 losses: 3.7881975173950195\n",
      "1 800 losses: 3.5731048583984375\n",
      "1 900 losses: 2.925558090209961\n",
      "Epoch 2, Loss: 3.47660756111145, Accuracy: 16.12400245666504, Test Loss: 3.205563545227051, Test Accuracy: 21.960006713867188\n",
      "2 0 losses: 3.364091157913208\n",
      "2 100 losses: 3.39127254486084\n",
      "2 200 losses: 3.4530022144317627\n",
      "2 300 losses: 3.064540386199951\n",
      "2 400 losses: 3.0310986042022705\n",
      "2 500 losses: 3.4381659030914307\n",
      "2 600 losses: 3.220228910446167\n",
      "2 700 losses: 3.3639025688171387\n",
      "2 800 losses: 3.0741524696350098\n",
      "2 900 losses: 2.525942802429199\n",
      "Epoch 3, Loss: 3.01709246635437, Accuracy: 24.822010040283203, Test Loss: 2.9128048419952393, Test Accuracy: 27.239986419677734\n",
      "3 0 losses: 3.026482343673706\n",
      "3 100 losses: 2.9839212894439697\n",
      "3 200 losses: 3.015681266784668\n",
      "3 300 losses: 2.725226402282715\n",
      "3 400 losses: 2.608146905899048\n",
      "3 500 losses: 2.872319221496582\n",
      "3 600 losses: 2.8627853393554688\n",
      "3 700 losses: 3.060556650161743\n",
      "3 800 losses: 2.7146172523498535\n",
      "3 900 losses: 2.1886796951293945\n",
      "Epoch 4, Loss: 2.6458256244659424, Accuracy: 32.281978607177734, Test Loss: 2.7606313228607178, Test Accuracy: 30.839988708496094\n",
      "4 0 losses: 2.5880279541015625\n",
      "4 100 losses: 2.6466314792633057\n",
      "4 200 losses: 2.552196979522705\n",
      "4 300 losses: 2.4004065990448\n",
      "4 400 losses: 2.105565071105957\n",
      "4 500 losses: 2.4155282974243164\n",
      "4 600 losses: 2.4305343627929688\n",
      "4 700 losses: 2.6675543785095215\n",
      "4 800 losses: 2.37318754196167\n",
      "4 900 losses: 1.83184015750885\n",
      "Epoch 5, Loss: 2.3069422245025635, Accuracy: 39.41401672363281, Test Loss: 2.750241279602051, Test Accuracy: 32.329986572265625\n",
      "5 0 losses: 2.286781072616577\n",
      "5 100 losses: 2.3344650268554688\n",
      "5 200 losses: 2.185688018798828\n",
      "5 300 losses: 2.190157651901245\n",
      "5 400 losses: 1.7750087976455688\n",
      "5 500 losses: 2.011887311935425\n",
      "5 600 losses: 2.0346384048461914\n",
      "5 700 losses: 2.248600959777832\n",
      "5 800 losses: 1.955591082572937\n",
      "5 900 losses: 1.5528007745742798\n",
      "Epoch 6, Loss: 2.001063585281372, Accuracy: 46.2080192565918, Test Loss: 2.7592480182647705, Test Accuracy: 33.049991607666016\n",
      "6 0 losses: 2.1064436435699463\n",
      "6 100 losses: 2.0842103958129883\n",
      "6 200 losses: 1.8219753503799438\n",
      "6 300 losses: 1.9137340784072876\n",
      "6 400 losses: 1.5250717401504517\n",
      "6 500 losses: 1.661702275276184\n",
      "6 600 losses: 1.715059518814087\n",
      "6 700 losses: 1.909385085105896\n",
      "6 800 losses: 1.6564828157424927\n",
      "6 900 losses: 1.2096859216690063\n",
      "Epoch 7, Loss: 1.7328788042068481, Accuracy: 52.16999816894531, Test Loss: 2.8985562324523926, Test Accuracy: 32.55998611450195\n",
      "7 0 losses: 1.7786020040512085\n",
      "7 100 losses: 1.7563024759292603\n",
      "7 200 losses: 1.5192553997039795\n",
      "7 300 losses: 1.7956727743148804\n",
      "7 400 losses: 1.3178209066390991\n",
      "7 500 losses: 1.301012396812439\n",
      "7 600 losses: 1.5489643812179565\n",
      "7 700 losses: 1.5631582736968994\n",
      "7 800 losses: 1.9456243515014648\n",
      "7 900 losses: 1.2931100130081177\n",
      "Epoch 8, Loss: 1.504152536392212, Accuracy: 57.43398666381836, Test Loss: 2.9879629611968994, Test Accuracy: 34.39999771118164\n",
      "8 0 losses: 1.4545880556106567\n",
      "8 100 losses: 1.6823852062225342\n",
      "8 200 losses: 1.6019786596298218\n",
      "8 300 losses: 1.7109997272491455\n",
      "8 400 losses: 1.0129395723342896\n",
      "8 500 losses: 1.0631948709487915\n",
      "8 600 losses: 1.3670363426208496\n",
      "8 700 losses: 1.4646470546722412\n",
      "8 800 losses: 1.3573144674301147\n",
      "8 900 losses: 0.64319908618927\n",
      "Epoch 9, Loss: 1.2810653448104858, Accuracy: 62.959922790527344, Test Loss: 3.00883150100708, Test Accuracy: 35.3699951171875\n",
      "9 0 losses: 1.1491914987564087\n",
      "9 100 losses: 1.3011889457702637\n",
      "9 200 losses: 1.3143692016601562\n",
      "9 300 losses: 1.5968295335769653\n",
      "9 400 losses: 0.9421699047088623\n",
      "9 500 losses: 0.6823452711105347\n",
      "9 600 losses: 0.8747574090957642\n",
      "9 700 losses: 1.2789100408554077\n",
      "9 800 losses: 1.3064513206481934\n",
      "9 900 losses: 0.9703448414802551\n",
      "Epoch 10, Loss: 1.0997824668884277, Accuracy: 67.7779312133789, Test Loss: 3.1949968338012695, Test Accuracy: 34.80998611450195\n",
      "10 0 losses: 0.9685206413269043\n",
      "10 100 losses: 1.1271837949752808\n",
      "10 200 losses: 1.0907093286514282\n",
      "10 300 losses: 0.9912889003753662\n",
      "10 400 losses: 0.759059727191925\n",
      "10 500 losses: 0.7689154148101807\n",
      "10 600 losses: 0.9671893119812012\n",
      "10 700 losses: 1.1035313606262207\n",
      "10 800 losses: 0.8740035891532898\n",
      "10 900 losses: 0.9291043281555176\n",
      "Epoch 11, Loss: 0.9067529439926147, Accuracy: 73.05406951904297, Test Loss: 3.253713607788086, Test Accuracy: 34.299991607666016\n",
      "11 0 losses: 0.9391065835952759\n",
      "11 100 losses: 0.8068162798881531\n",
      "11 200 losses: 0.6968708634376526\n",
      "11 300 losses: 1.1084457635879517\n",
      "11 400 losses: 0.6076958179473877\n",
      "11 500 losses: 0.6224226355552673\n",
      "11 600 losses: 0.5183849334716797\n",
      "11 700 losses: 1.1597018241882324\n",
      "11 800 losses: 0.7966676950454712\n",
      "11 900 losses: 0.597165584564209\n",
      "Epoch 12, Loss: 0.724980354309082, Accuracy: 77.9320068359375, Test Loss: 3.638699531555176, Test Accuracy: 33.95998764038086\n",
      "12 0 losses: 0.8884164690971375\n",
      "12 100 losses: 1.0066652297973633\n",
      "12 200 losses: 0.4325982332229614\n",
      "12 300 losses: 0.7276453375816345\n",
      "12 400 losses: 0.37965089082717896\n",
      "12 500 losses: 0.4372587502002716\n",
      "12 600 losses: 0.5483882427215576\n",
      "12 700 losses: 0.4056858718395233\n",
      "12 800 losses: 0.43064889311790466\n",
      "12 900 losses: 0.45812705159187317\n",
      "Epoch 13, Loss: 0.5897129774093628, Accuracy: 81.77611541748047, Test Loss: 3.952298641204834, Test Accuracy: 35.729976654052734\n",
      "13 0 losses: 0.5653639435768127\n",
      "13 100 losses: 0.7522400617599487\n",
      "13 200 losses: 0.46558311581611633\n",
      "13 300 losses: 0.4022456407546997\n",
      "13 400 losses: 0.5346876382827759\n",
      "13 500 losses: 0.30557218194007874\n",
      "13 600 losses: 0.3606950342655182\n",
      "13 700 losses: 0.2520195245742798\n",
      "13 800 losses: 0.5017147064208984\n",
      "13 900 losses: 0.2663944661617279\n",
      "Epoch 14, Loss: 0.46318838000297546, Accuracy: 85.6441879272461, Test Loss: 4.429152011871338, Test Accuracy: 35.8699836730957\n",
      "14 0 losses: 0.4241996109485626\n",
      "14 100 losses: 0.4812414050102234\n",
      "14 200 losses: 0.5010004639625549\n",
      "14 300 losses: 0.32940101623535156\n",
      "14 400 losses: 0.5231865644454956\n",
      "14 500 losses: 0.32809799909591675\n",
      "14 600 losses: 0.27714577317237854\n",
      "14 700 losses: 0.6883999705314636\n",
      "14 800 losses: 0.45973289012908936\n",
      "14 900 losses: 0.14683622121810913\n",
      "Epoch 15, Loss: 0.3694262206554413, Accuracy: 88.26016998291016, Test Loss: 4.59433650970459, Test Accuracy: 36.27998733520508\n",
      "15 0 losses: 0.35681653022766113\n",
      "15 100 losses: 0.3729689419269562\n",
      "15 200 losses: 0.35708752274513245\n",
      "15 300 losses: 0.5487608909606934\n",
      "15 400 losses: 0.12341544777154922\n",
      "15 500 losses: 0.14876726269721985\n",
      "15 600 losses: 0.3121417164802551\n",
      "15 700 losses: 0.3838407099246979\n",
      "15 800 losses: 0.16050247848033905\n",
      "15 900 losses: 0.2402118444442749\n",
      "Epoch 16, Loss: 0.3145581781864166, Accuracy: 90.10820007324219, Test Loss: 4.555108070373535, Test Accuracy: 37.119972229003906\n",
      "16 0 losses: 0.3647884726524353\n",
      "16 100 losses: 0.2742820084095001\n",
      "16 200 losses: 0.20622371137142181\n",
      "16 300 losses: 0.21924825012683868\n",
      "16 400 losses: 0.07230517268180847\n",
      "16 500 losses: 0.17835690081119537\n",
      "16 600 losses: 0.2987188994884491\n",
      "16 700 losses: 0.32283321022987366\n",
      "16 800 losses: 0.30790165066719055\n",
      "16 900 losses: 0.2563830614089966\n",
      "Epoch 17, Loss: 0.26453107595443726, Accuracy: 91.71822357177734, Test Loss: 4.825491905212402, Test Accuracy: 37.219970703125\n",
      "17 0 losses: 0.16753749549388885\n",
      "17 100 losses: 0.13954678177833557\n",
      "17 200 losses: 0.2721571922302246\n",
      "17 300 losses: 0.1765478551387787\n",
      "17 400 losses: 0.20026756823062897\n",
      "17 500 losses: 0.36109259724617004\n",
      "17 600 losses: 0.22562819719314575\n",
      "17 700 losses: 0.2364216297864914\n",
      "17 800 losses: 0.17418406903743744\n",
      "17 900 losses: 0.15052425861358643\n",
      "Epoch 18, Loss: 0.22393113374710083, Accuracy: 92.97415924072266, Test Loss: 5.039994716644287, Test Accuracy: 37.859989166259766\n",
      "18 0 losses: 0.2753824293613434\n",
      "18 100 losses: 0.25622403621673584\n",
      "18 200 losses: 0.3016239106655121\n",
      "18 300 losses: 0.3742539584636688\n",
      "18 400 losses: 0.1169978678226471\n",
      "18 500 losses: 0.21687132120132446\n",
      "18 600 losses: 0.32482945919036865\n",
      "18 700 losses: 0.06080789491534233\n",
      "18 800 losses: 0.16331954300403595\n",
      "18 900 losses: 0.2494681179523468\n",
      "Epoch 19, Loss: 0.19785214960575104, Accuracy: 93.7161865234375, Test Loss: 5.286087512969971, Test Accuracy: 37.319976806640625\n",
      "19 0 losses: 0.120175801217556\n",
      "19 100 losses: 0.1283198893070221\n",
      "19 200 losses: 0.25863903760910034\n",
      "19 300 losses: 0.06799910217523575\n",
      "19 400 losses: 0.12485402077436447\n",
      "19 500 losses: 0.0794098824262619\n",
      "19 600 losses: 0.053961679339408875\n",
      "19 700 losses: 0.10326719284057617\n",
      "19 800 losses: 0.14856480062007904\n",
      "19 900 losses: 0.1872568279504776\n",
      "Epoch 20, Loss: 0.179491326212883, Accuracy: 94.318115234375, Test Loss: 4.661709785461426, Test Accuracy: 38.759986877441406\n",
      "20 0 losses: 0.0934079959988594\n",
      "20 100 losses: 0.10191048681735992\n",
      "20 200 losses: 0.27533480525016785\n",
      "20 300 losses: 0.2459784746170044\n",
      "20 400 losses: 0.13179494440555573\n",
      "20 500 losses: 0.39458075165748596\n",
      "20 600 losses: 0.25448787212371826\n",
      "20 700 losses: 0.3251751959323883\n",
      "20 800 losses: 0.13976670801639557\n",
      "20 900 losses: 0.16704224050045013\n",
      "Epoch 21, Loss: 0.1664503514766693, Accuracy: 94.68014526367188, Test Loss: 4.805813789367676, Test Accuracy: 38.989994049072266\n",
      "21 0 losses: 0.14230452477931976\n",
      "21 100 losses: 0.0704091340303421\n",
      "21 200 losses: 0.24145390093326569\n",
      "21 300 losses: 0.16562269628047943\n",
      "21 400 losses: 0.0719798132777214\n",
      "21 500 losses: 0.09438998997211456\n",
      "21 600 losses: 0.1788545548915863\n",
      "21 700 losses: 0.13591426610946655\n",
      "21 800 losses: 0.1574520617723465\n",
      "21 900 losses: 0.08138036727905273\n",
      "Epoch 22, Loss: 0.144037663936615, Accuracy: 95.39814758300781, Test Loss: 5.033500671386719, Test Accuracy: 38.790008544921875\n",
      "22 0 losses: 0.19489599764347076\n",
      "22 100 losses: 0.187056764960289\n",
      "22 200 losses: 0.26748988032341003\n",
      "22 300 losses: 0.10786635428667068\n",
      "22 400 losses: 0.4458804428577423\n",
      "22 500 losses: 0.18980586528778076\n",
      "22 600 losses: 0.07266334444284439\n",
      "22 700 losses: 0.2413375973701477\n",
      "22 800 losses: 0.0660172551870346\n",
      "22 900 losses: 0.2786532938480377\n",
      "Epoch 23, Loss: 0.1369064599275589, Accuracy: 95.76409912109375, Test Loss: 5.184738636016846, Test Accuracy: 38.37998962402344\n",
      "23 0 losses: 0.07024119049310684\n",
      "23 100 losses: 0.24723714590072632\n",
      "23 200 losses: 0.16647456586360931\n",
      "23 300 losses: 0.078071229159832\n",
      "23 400 losses: 0.05741691589355469\n",
      "23 500 losses: 0.03482327610254288\n",
      "23 600 losses: 0.10990690439939499\n",
      "23 700 losses: 0.16171851754188538\n",
      "23 800 losses: 0.19725610315799713\n",
      "23 900 losses: 0.11796925216913223\n",
      "Epoch 24, Loss: 0.1312757134437561, Accuracy: 95.93412017822266, Test Loss: 4.928979873657227, Test Accuracy: 38.29998779296875\n",
      "24 0 losses: 0.07807978242635727\n",
      "24 100 losses: 0.11282090842723846\n",
      "24 200 losses: 0.3546312749385834\n",
      "24 300 losses: 0.03408069536089897\n",
      "24 400 losses: 0.08620834350585938\n",
      "24 500 losses: 0.15186022222042084\n",
      "24 600 losses: 0.12585949897766113\n",
      "24 700 losses: 0.03145267814397812\n",
      "24 800 losses: 0.10942234843969345\n",
      "24 900 losses: 0.22195780277252197\n",
      "Epoch 25, Loss: 0.11844710260629654, Accuracy: 96.27802276611328, Test Loss: 5.160114765167236, Test Accuracy: 38.43999481201172\n",
      "25 0 losses: 0.1315280795097351\n",
      "25 100 losses: 0.2982042729854584\n",
      "25 200 losses: 0.050986189395189285\n",
      "25 300 losses: 0.18304850161075592\n",
      "25 400 losses: 0.0684400349855423\n",
      "25 500 losses: 0.1424470990896225\n",
      "25 600 losses: 0.12322904914617538\n",
      "25 700 losses: 0.28831055760383606\n",
      "25 800 losses: 0.08245978504419327\n",
      "25 900 losses: 0.17649593949317932\n",
      "Epoch 26, Loss: 0.11716923117637634, Accuracy: 96.27605438232422, Test Loss: 5.003143787384033, Test Accuracy: 38.99998474121094\n",
      "26 0 losses: 0.14073233306407928\n",
      "26 100 losses: 0.15561924874782562\n",
      "26 200 losses: 0.043426137417554855\n",
      "26 300 losses: 0.07167598605155945\n",
      "26 400 losses: 0.24473632872104645\n",
      "26 500 losses: 0.22763992846012115\n",
      "26 600 losses: 0.24180753529071808\n",
      "26 700 losses: 0.1794847846031189\n",
      "26 800 losses: 0.1698455661535263\n",
      "26 900 losses: 0.11066605895757675\n",
      "Epoch 27, Loss: 0.11091013997793198, Accuracy: 96.49603271484375, Test Loss: 5.255926609039307, Test Accuracy: 38.16999053955078\n",
      "27 0 losses: 0.15706735849380493\n",
      "27 100 losses: 0.0816265270113945\n",
      "27 200 losses: 0.4689084589481354\n",
      "27 300 losses: 0.048686135560274124\n",
      "27 400 losses: 0.019102180376648903\n",
      "27 500 losses: 0.03795284405350685\n",
      "27 600 losses: 0.09540799260139465\n",
      "27 700 losses: 0.0877578929066658\n",
      "27 800 losses: 0.09950147569179535\n",
      "27 900 losses: 0.048043143004179\n",
      "Epoch 28, Loss: 0.10246843099594116, Accuracy: 96.85599517822266, Test Loss: 5.223968505859375, Test Accuracy: 38.670005798339844\n",
      "28 0 losses: 0.058244358748197556\n",
      "28 100 losses: 0.09266562759876251\n",
      "28 200 losses: 0.0784647986292839\n",
      "28 300 losses: 0.3063848614692688\n",
      "28 400 losses: 0.012484092265367508\n",
      "28 500 losses: 0.05111842229962349\n",
      "28 600 losses: 0.022195519879460335\n",
      "28 700 losses: 0.08635815978050232\n",
      "28 800 losses: 0.03488025441765785\n",
      "28 900 losses: 0.05142144113779068\n",
      "Epoch 29, Loss: 0.10161351412534714, Accuracy: 96.87397766113281, Test Loss: 5.100801944732666, Test Accuracy: 38.85997772216797\n",
      "29 0 losses: 0.34497207403182983\n",
      "29 100 losses: 0.04206635802984238\n",
      "29 200 losses: 0.0721568763256073\n",
      "29 300 losses: 0.18343326449394226\n",
      "29 400 losses: 0.0799332782626152\n",
      "29 500 losses: 0.06879174709320068\n",
      "29 600 losses: 0.0833350121974945\n",
      "29 700 losses: 0.023435743525624275\n",
      "29 800 losses: 0.3770792484283447\n",
      "29 900 losses: 0.04976265877485275\n",
      "Epoch 30, Loss: 0.09977348148822784, Accuracy: 96.86795806884766, Test Loss: 4.919356822967529, Test Accuracy: 38.24998474121094\n",
      "30 0 losses: 0.058014992624521255\n",
      "30 100 losses: 0.04678356274962425\n",
      "30 200 losses: 0.1252000778913498\n",
      "30 300 losses: 0.24729511141777039\n",
      "30 400 losses: 0.09360279887914658\n",
      "30 500 losses: 0.1710817515850067\n",
      "30 600 losses: 0.0809861347079277\n",
      "30 700 losses: 0.1049283966422081\n",
      "30 800 losses: 0.03660478815436363\n",
      "30 900 losses: 0.0251803919672966\n",
      "Epoch 31, Loss: 0.08756991475820541, Accuracy: 97.24198913574219, Test Loss: 5.311678409576416, Test Accuracy: 38.7299919128418\n",
      "31 0 losses: 0.041585393249988556\n",
      "31 100 losses: 0.1569499373435974\n",
      "31 200 losses: 0.06436219066381454\n",
      "31 300 losses: 0.11518041789531708\n",
      "31 400 losses: 0.015032213181257248\n",
      "31 500 losses: 0.11142133921384811\n",
      "31 600 losses: 0.16943638026714325\n",
      "31 700 losses: 0.09636907279491425\n",
      "31 800 losses: 0.04491148516535759\n",
      "31 900 losses: 0.0975697785615921\n",
      "Epoch 32, Loss: 0.09129748493432999, Accuracy: 97.19393920898438, Test Loss: 5.128274440765381, Test Accuracy: 39.249996185302734\n",
      "32 0 losses: 0.05856272205710411\n",
      "32 100 losses: 0.026350252330303192\n",
      "32 200 losses: 0.03441861271858215\n",
      "32 300 losses: 0.032269325107336044\n",
      "32 400 losses: 0.06854860484600067\n",
      "32 500 losses: 0.05578826367855072\n",
      "32 600 losses: 0.15887926518917084\n",
      "32 700 losses: 0.05682927742600441\n",
      "32 800 losses: 0.020798495039343834\n",
      "32 900 losses: 0.08583913743495941\n",
      "Epoch 33, Loss: 0.08561829477548599, Accuracy: 97.33601379394531, Test Loss: 4.560596466064453, Test Accuracy: 38.99999237060547\n",
      "33 0 losses: 0.05566958338022232\n",
      "33 100 losses: 0.12204598635435104\n",
      "33 200 losses: 0.05710570886731148\n",
      "33 300 losses: 0.07204756885766983\n",
      "33 400 losses: 0.05798560753464699\n",
      "33 500 losses: 0.01092023216187954\n",
      "33 600 losses: 0.08457458764314651\n",
      "33 700 losses: 0.08937130868434906\n",
      "33 800 losses: 0.061037950217723846\n",
      "33 900 losses: 0.18312762677669525\n",
      "Epoch 34, Loss: 0.08228986710309982, Accuracy: 97.42201232910156, Test Loss: 4.828990936279297, Test Accuracy: 38.94998550415039\n",
      "34 0 losses: 0.08683528751134872\n",
      "34 100 losses: 0.10687630623579025\n",
      "34 200 losses: 0.05180484801530838\n",
      "34 300 losses: 0.14232726395130157\n",
      "34 400 losses: 0.05074566230177879\n",
      "34 500 losses: 0.03717515990138054\n",
      "34 600 losses: 0.05442983657121658\n",
      "34 700 losses: 0.041935987770557404\n",
      "34 800 losses: 0.06158328056335449\n",
      "34 900 losses: 0.01476398203521967\n",
      "Epoch 35, Loss: 0.08266059309244156, Accuracy: 97.42193603515625, Test Loss: 4.871893405914307, Test Accuracy: 40.20998764038086\n",
      "35 0 losses: 0.010523837059736252\n",
      "35 100 losses: 0.14591817557811737\n",
      "35 200 losses: 0.2835893929004669\n",
      "35 300 losses: 0.04164627939462662\n",
      "35 400 losses: 0.07023583352565765\n",
      "35 500 losses: 0.052982062101364136\n",
      "35 600 losses: 0.035319551825523376\n",
      "35 700 losses: 0.040316056460142136\n",
      "35 800 losses: 0.040173135697841644\n",
      "35 900 losses: 0.23675686120986938\n",
      "Epoch 36, Loss: 0.07636334002017975, Accuracy: 97.7019271850586, Test Loss: 4.901005744934082, Test Accuracy: 39.2400016784668\n",
      "36 0 losses: 0.090176060795784\n",
      "36 100 losses: 0.01109199970960617\n",
      "36 200 losses: 0.13941729068756104\n",
      "36 300 losses: 0.021385885775089264\n",
      "36 400 losses: 0.04164145141839981\n",
      "36 500 losses: 0.21669693291187286\n",
      "36 600 losses: 0.0474553219974041\n",
      "36 700 losses: 0.031680233776569366\n",
      "36 800 losses: 0.15829776227474213\n",
      "36 900 losses: 0.029968375340104103\n",
      "Epoch 37, Loss: 0.07314495742321014, Accuracy: 97.78585052490234, Test Loss: 4.999079704284668, Test Accuracy: 38.429996490478516\n",
      "37 0 losses: 0.08919055014848709\n",
      "37 100 losses: 0.05301749333739281\n",
      "37 200 losses: 0.009960111230611801\n",
      "37 300 losses: 0.009095794521272182\n",
      "37 400 losses: 0.0638858824968338\n",
      "37 500 losses: 0.01215228158980608\n",
      "37 600 losses: 0.11653178930282593\n",
      "37 700 losses: 0.031922437250614166\n",
      "37 800 losses: 0.05602999031543732\n",
      "37 900 losses: 0.1379563957452774\n",
      "Epoch 38, Loss: 0.0735122486948967, Accuracy: 97.73789978027344, Test Loss: 5.010414123535156, Test Accuracy: 39.68999099731445\n",
      "38 0 losses: 0.0756133571267128\n",
      "38 100 losses: 0.12258260697126389\n",
      "38 200 losses: 0.047354668378829956\n",
      "38 300 losses: 0.013452024199068546\n",
      "38 400 losses: 0.1537991762161255\n",
      "38 500 losses: 0.03185991197824478\n",
      "38 600 losses: 0.04235845431685448\n",
      "38 700 losses: 0.03064187802374363\n",
      "38 800 losses: 0.06674766540527344\n",
      "38 900 losses: 0.04079752415418625\n",
      "Epoch 39, Loss: 0.06773851811885834, Accuracy: 97.88793182373047, Test Loss: 5.107690811157227, Test Accuracy: 39.959991455078125\n",
      "39 0 losses: 0.053089383989572525\n",
      "39 100 losses: 0.03503546863794327\n",
      "39 200 losses: 0.02242906391620636\n",
      "39 300 losses: 0.014906013384461403\n",
      "39 400 losses: 0.13221056759357452\n",
      "39 500 losses: 0.18088680505752563\n",
      "39 600 losses: 0.04642496630549431\n",
      "39 700 losses: 0.09759283810853958\n",
      "39 800 losses: 0.2547725737094879\n",
      "39 900 losses: 0.010589445009827614\n",
      "Epoch 40, Loss: 0.07097320258617401, Accuracy: 97.77391815185547, Test Loss: 4.772340774536133, Test Accuracy: 38.85999298095703\n",
      "40 0 losses: 0.28598839044570923\n",
      "40 100 losses: 0.09262925386428833\n",
      "40 200 losses: 0.017276190221309662\n",
      "40 300 losses: 0.21131223440170288\n",
      "40 400 losses: 0.07102597504854202\n",
      "40 500 losses: 0.38097772002220154\n",
      "40 600 losses: 0.1497611552476883\n",
      "40 700 losses: 0.017902176827192307\n",
      "40 800 losses: 0.06690286099910736\n",
      "40 900 losses: 0.022636661306023598\n",
      "Epoch 41, Loss: 0.06627390533685684, Accuracy: 97.9819564819336, Test Loss: 4.850552082061768, Test Accuracy: 40.05000305175781\n",
      "41 0 losses: 0.02138340286910534\n",
      "41 100 losses: 0.060799919068813324\n",
      "41 200 losses: 0.015722980722784996\n",
      "41 300 losses: 0.009752893820405006\n",
      "41 400 losses: 0.08187807351350784\n",
      "41 500 losses: 0.041192978620529175\n",
      "41 600 losses: 0.02837086282670498\n",
      "41 700 losses: 0.18292874097824097\n",
      "41 800 losses: 0.02332345023751259\n",
      "41 900 losses: 0.02654743194580078\n",
      "Epoch 42, Loss: 0.0673551857471466, Accuracy: 97.98994445800781, Test Loss: 4.691527366638184, Test Accuracy: 39.779991149902344\n",
      "42 0 losses: 0.12852570414543152\n",
      "42 100 losses: 0.00566895492374897\n",
      "42 200 losses: 0.13327333331108093\n",
      "42 300 losses: 0.010916913859546185\n",
      "42 400 losses: 0.031147213652729988\n",
      "42 500 losses: 0.024335991591215134\n",
      "42 600 losses: 0.007228671107441187\n",
      "42 700 losses: 0.0087586073204875\n",
      "42 800 losses: 0.07304593920707703\n",
      "42 900 losses: 0.011947175487875938\n",
      "Epoch 43, Loss: 0.06431984901428223, Accuracy: 98.08995056152344, Test Loss: 5.397733688354492, Test Accuracy: 39.339996337890625\n",
      "43 0 losses: 0.010338550433516502\n",
      "43 100 losses: 0.006831218022853136\n",
      "43 200 losses: 0.2486771047115326\n",
      "43 300 losses: 0.07009053230285645\n",
      "43 400 losses: 0.04264964163303375\n",
      "43 500 losses: 0.04355064034461975\n",
      "43 600 losses: 0.09361903369426727\n",
      "43 700 losses: 0.08667629957199097\n",
      "43 800 losses: 0.06010443717241287\n",
      "43 900 losses: 0.1118604838848114\n",
      "Epoch 44, Loss: 0.061542101204395294, Accuracy: 98.1138687133789, Test Loss: 4.869419097900391, Test Accuracy: 40.62000274658203\n",
      "44 0 losses: 0.018883531913161278\n",
      "44 100 losses: 0.05773983895778656\n",
      "44 200 losses: 0.011325540952384472\n",
      "44 300 losses: 0.08048468828201294\n",
      "44 400 losses: 0.005696241743862629\n",
      "44 500 losses: 0.031281065195798874\n",
      "44 600 losses: 0.13735589385032654\n",
      "44 700 losses: 0.1353694647550583\n",
      "44 800 losses: 0.007568748202174902\n",
      "44 900 losses: 0.009832127951085567\n",
      "Epoch 45, Loss: 0.05973878502845764, Accuracy: 98.17591094970703, Test Loss: 4.94221305847168, Test Accuracy: 39.739994049072266\n",
      "45 0 losses: 0.02247067727148533\n",
      "45 100 losses: 0.10872602462768555\n",
      "45 200 losses: 0.031223101541399956\n",
      "45 300 losses: 0.04313648119568825\n",
      "45 400 losses: 0.07938815653324127\n",
      "45 500 losses: 0.036552976816892624\n",
      "45 600 losses: 0.017613792791962624\n",
      "45 700 losses: 0.03461529314517975\n",
      "45 800 losses: 0.026834672316908836\n",
      "45 900 losses: 0.012148873880505562\n",
      "Epoch 46, Loss: 0.05876115337014198, Accuracy: 98.16192626953125, Test Loss: 5.151177883148193, Test Accuracy: 39.34996795654297\n",
      "46 0 losses: 0.04391305893659592\n",
      "46 100 losses: 0.07289084792137146\n",
      "46 200 losses: 0.1761775016784668\n",
      "46 300 losses: 0.23499475419521332\n",
      "46 400 losses: 0.01619543507695198\n",
      "46 500 losses: 0.10065975785255432\n",
      "46 600 losses: 0.036049481481313705\n",
      "46 700 losses: 0.01832675375044346\n",
      "46 800 losses: 0.013756148517131805\n",
      "46 900 losses: 0.04637628421187401\n",
      "Epoch 47, Loss: 0.05717368796467781, Accuracy: 98.26790618896484, Test Loss: 4.694287300109863, Test Accuracy: 40.37998580932617\n",
      "47 0 losses: 0.08838504552841187\n",
      "47 100 losses: 0.038964323699474335\n",
      "47 200 losses: 0.12113096565008163\n",
      "47 300 losses: 0.0095744077116251\n",
      "47 400 losses: 0.019949795678257942\n",
      "47 500 losses: 0.01366020180284977\n",
      "47 600 losses: 0.04423685371875763\n",
      "47 700 losses: 0.22499486804008484\n",
      "47 800 losses: 0.010581948794424534\n",
      "47 900 losses: 0.07053607702255249\n",
      "Epoch 48, Loss: 0.0546511635184288, Accuracy: 98.27191925048828, Test Loss: 4.765923976898193, Test Accuracy: 40.699974060058594\n",
      "48 0 losses: 0.10633441805839539\n",
      "48 100 losses: 0.05168621987104416\n",
      "48 200 losses: 0.005967787001281977\n",
      "48 300 losses: 0.09961304813623428\n",
      "48 400 losses: 0.1401418149471283\n",
      "48 500 losses: 0.07869956642389297\n",
      "48 600 losses: 0.012075244449079037\n",
      "48 700 losses: 0.006195696536451578\n",
      "48 800 losses: 0.06599902361631393\n",
      "48 900 losses: 0.06114315986633301\n",
      "Epoch 49, Loss: 0.05083892121911049, Accuracy: 98.42385864257812, Test Loss: 4.791995048522949, Test Accuracy: 40.12997817993164\n",
      "49 0 losses: 0.021135427057743073\n",
      "49 100 losses: 0.011903095990419388\n",
      "49 200 losses: 0.06297703087329865\n",
      "49 300 losses: 0.05893298611044884\n",
      "49 400 losses: 0.007404184900224209\n",
      "49 500 losses: 0.02321663312613964\n",
      "49 600 losses: 0.04915357753634453\n",
      "49 700 losses: 0.04692910984158516\n",
      "49 800 losses: 0.07727499306201935\n",
      "49 900 losses: 0.008373161777853966\n",
      "Epoch 50, Loss: 0.05692682042717934, Accuracy: 98.30187225341797, Test Loss: 5.023319244384766, Test Accuracy: 40.73997497558594\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-85c355ec80e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'baseline.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m   1999\u001b[0m     \"\"\"\n\u001b[0;32m   2000\u001b[0m     \u001b[1;31m# pylint: enable=line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2001\u001b[1;33m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0m\u001b[0;32m   2002\u001b[0m                     signatures, options, save_traces)\n\u001b[0;32m   2003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m    144\u001b[0m     if (not model._is_graph_network and  # pylint:disable=protected-access\n\u001b[0;32m    145\u001b[0m         not isinstance(model, sequential.Sequential)):\n\u001b[1;32m--> 146\u001b[1;33m       raise NotImplementedError(\n\u001b[0m\u001b[0;32m    147\u001b[0m           \u001b[1;34m'Saving the model to HDF5 format requires the model to be a '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m           \u001b[1;34m'Functional model or a Sequential model. It does not work for '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`."
     ]
    }
   ],
   "source": [
    "model.build(input_shape=(None,32,32,3))\n",
    "lr = 1e-4\n",
    "optimizer=optimizers.Adam(lr=lr)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "train_accuracy = tf.keras.metrics.Mean('train_accuracy', dtype=tf.float32)\n",
    "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
    "test_accuracy = tf.keras.metrics.Mean('test_accuracy', dtype=tf.float32)\n",
    "\n",
    "for epoch in range(50):\n",
    "    # if epoch % 5 == 4:\n",
    "    #     lr/=10\n",
    "    #     optimizer.lr = lr\n",
    "    for step,(x,y) in enumerate(train_db):\n",
    "        #这里做一个前向循环,将需要求解梯度放进来\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_onehot=tf.one_hot(y,depth=100)\n",
    "            # x, y_onehot = cutmix_mask(x,y_onehot)\n",
    "            #[b,32,32,3] => [b,100]\n",
    "            logits=model(x)\n",
    "            #[b] => [b,100]\n",
    "            #compute loss\n",
    "            loss=tf.losses.categorical_crossentropy(y_onehot,logits)\n",
    "    \n",
    "            loss=tf.reduce_mean(loss)\n",
    "            \n",
    "        pred=tf.argmax(logits,axis=1)\n",
    "        pred=tf.cast(pred,dtype=tf.int32)\n",
    "        correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n",
    "        correct=tf.reduce_sum(correct)\n",
    "        acc = correct/x.shape[0]\n",
    "        \n",
    "        train_loss(loss)\n",
    "        train_accuracy(acc)\n",
    "        #计算gradient\n",
    "        grads=tape.gradient(loss,model.trainable_variables)\n",
    "        #传给优化器两个参数：grads和variable，完成梯度更新\n",
    "        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(epoch,step,'losses:',float(loss))\n",
    "            \n",
    "    \n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "        \n",
    "    total_num=0\n",
    "    total_correct=0\n",
    "    for x,y in test_db:\n",
    "        logits=model(x)\n",
    "        y_onehot = tf.one_hot(y,depth = 100)\n",
    "        loss=tf.losses.categorical_crossentropy(y_onehot,logits)\n",
    "        loss=tf.reduce_mean(loss)\n",
    "        \n",
    "        #prob=tf.nn.softmax(logits,axis=1)\n",
    "        pred=tf.argmax(logits,axis=1)\n",
    "        pred=tf.cast(pred,dtype=tf.int32)\n",
    "        correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n",
    "        correct=tf.reduce_sum(correct)\n",
    "\n",
    "        total_num += x.shape[0]\n",
    "        total_correct += int(correct)\n",
    "        \n",
    "        test_accuracy(correct/x.shape[0])\n",
    "        test_loss(loss)\n",
    "    \n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "            \n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                         train_loss.result(), \n",
    "                         train_accuracy.result()*100,\n",
    "                         test_loss.result(), \n",
    "                         test_accuracy.result()*100))\n",
    "\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "model.save_weights('baseline.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('baseline.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 losses: 4.6083478927612305\n",
      "0 100 losses: 4.606198310852051\n",
      "0 200 losses: 4.415788650512695\n",
      "0 300 losses: 4.3008575439453125\n",
      "0 400 losses: 4.232976913452148\n",
      "0 500 losses: 4.002837657928467\n",
      "0 600 losses: 4.1679816246032715\n",
      "0 700 losses: 3.956101417541504\n",
      "0 800 losses: 4.018930435180664\n",
      "0 900 losses: 3.664863348007202\n",
      "Epoch 1, Loss: 4.088409423828125, Accuracy: 6.170012950897217, Test Loss: 3.7066118717193604, Test Accuracy: 12.080011367797852\n",
      "1 0 losses: 3.8980062007904053\n",
      "1 100 losses: 3.8947360515594482\n",
      "1 200 losses: 3.9306674003601074\n",
      "1 300 losses: 3.775172710418701\n",
      "1 400 losses: 3.618562936782837\n",
      "1 500 losses: 3.704801082611084\n",
      "1 600 losses: 3.5955889225006104\n",
      "1 700 losses: 3.7246532440185547\n",
      "1 800 losses: 3.484116792678833\n",
      "1 900 losses: 3.016052007675171\n",
      "Epoch 2, Loss: 3.547438859939575, Accuracy: 14.948015213012695, Test Loss: 3.257460355758667, Test Accuracy: 20.910005569458008\n",
      "2 0 losses: 3.5213677883148193\n",
      "2 100 losses: 3.461047887802124\n",
      "2 200 losses: 3.5837273597717285\n",
      "2 300 losses: 3.352043867111206\n",
      "2 400 losses: 3.298081874847412\n",
      "2 500 losses: 3.457383632659912\n",
      "2 600 losses: 3.2371294498443604\n",
      "2 700 losses: 3.4440886974334717\n",
      "2 800 losses: 3.228511333465576\n",
      "2 900 losses: 2.440882682800293\n",
      "Epoch 3, Loss: 3.151945114135742, Accuracy: 22.24202537536621, Test Loss: 2.986274003982544, Test Accuracy: 25.920013427734375\n",
      "3 0 losses: 3.1289258003234863\n",
      "3 100 losses: 3.139709711074829\n",
      "3 200 losses: 3.0712690353393555\n",
      "3 300 losses: 2.92539644241333\n",
      "3 400 losses: 2.672819137573242\n",
      "3 500 losses: 3.0698206424713135\n",
      "3 600 losses: 2.8021085262298584\n",
      "3 700 losses: 3.038961410522461\n",
      "3 800 losses: 3.0172007083892822\n",
      "3 900 losses: 2.3641579151153564\n",
      "Epoch 4, Loss: 2.8153746128082275, Accuracy: 28.949979782104492, Test Loss: 2.7852118015289307, Test Accuracy: 30.60999298095703\n",
      "4 0 losses: 2.8145666122436523\n",
      "4 100 losses: 2.901993989944458\n",
      "4 200 losses: 2.6990182399749756\n",
      "4 300 losses: 2.7704825401306152\n",
      "4 400 losses: 2.259035110473633\n",
      "4 500 losses: 2.8011529445648193\n",
      "4 600 losses: 2.58381724357605\n",
      "4 700 losses: 2.5488274097442627\n",
      "4 800 losses: 2.7471206188201904\n",
      "4 900 losses: 2.141357183456421\n",
      "Epoch 5, Loss: 2.5279290676116943, Accuracy: 34.89994812011719, Test Loss: 2.74056339263916, Test Accuracy: 32.68998718261719\n",
      "5 0 losses: 2.597553014755249\n",
      "5 100 losses: 2.554208755493164\n",
      "5 200 losses: 2.560300827026367\n",
      "5 300 losses: 2.5071306228637695\n",
      "5 400 losses: 2.1912174224853516\n",
      "5 500 losses: 2.582125186920166\n",
      "5 600 losses: 2.339381456375122\n",
      "5 700 losses: 2.4480934143066406\n",
      "5 800 losses: 2.514272689819336\n",
      "5 900 losses: 1.9089486598968506\n",
      "Epoch 6, Loss: 2.259556770324707, Accuracy: 40.627967834472656, Test Loss: 2.685945749282837, Test Accuracy: 34.62998580932617\n",
      "6 0 losses: 2.343432664871216\n",
      "6 100 losses: 2.504195213317871\n",
      "6 200 losses: 2.228727340698242\n",
      "6 300 losses: 2.308396339416504\n",
      "6 400 losses: 1.8952490091323853\n",
      "6 500 losses: 2.15147066116333\n",
      "6 600 losses: 1.9255726337432861\n",
      "6 700 losses: 2.0673439502716064\n",
      "6 800 losses: 2.225191354751587\n",
      "6 900 losses: 1.5546867847442627\n",
      "Epoch 7, Loss: 1.9928820133209229, Accuracy: 46.64802551269531, Test Loss: 2.675428867340088, Test Accuracy: 36.77998352050781\n",
      "7 0 losses: 1.9495891332626343\n",
      "7 100 losses: 2.282999038696289\n",
      "7 200 losses: 2.0796148777008057\n",
      "7 300 losses: 2.290117025375366\n",
      "7 400 losses: 1.6907830238342285\n",
      "7 500 losses: 2.0289194583892822\n",
      "7 600 losses: 1.6698211431503296\n",
      "7 700 losses: 1.7617928981781006\n",
      "7 800 losses: 2.1092684268951416\n",
      "7 900 losses: 1.3526660203933716\n",
      "Epoch 8, Loss: 1.7541999816894531, Accuracy: 52.21995162963867, Test Loss: 2.7663919925689697, Test Accuracy: 36.079986572265625\n",
      "8 0 losses: 1.7566554546356201\n",
      "8 100 losses: 1.8345392942428589\n",
      "8 200 losses: 2.032405376434326\n",
      "8 300 losses: 1.9549951553344727\n",
      "8 400 losses: 1.572898268699646\n",
      "8 500 losses: 1.7126013040542603\n",
      "8 600 losses: 1.5531344413757324\n",
      "8 700 losses: 1.6392853260040283\n",
      "8 800 losses: 1.779668927192688\n",
      "8 900 losses: 1.0483813285827637\n",
      "Epoch 9, Loss: 1.5532456636428833, Accuracy: 57.175994873046875, Test Loss: 2.9352948665618896, Test Accuracy: 36.33998107910156\n",
      "9 0 losses: 1.590775728225708\n",
      "9 100 losses: 1.6383767127990723\n",
      "9 200 losses: 1.693976640701294\n",
      "9 300 losses: 1.8057361841201782\n",
      "9 400 losses: 1.0779755115509033\n",
      "9 500 losses: 1.4559887647628784\n",
      "9 600 losses: 1.301767110824585\n",
      "9 700 losses: 1.4957566261291504\n",
      "9 800 losses: 1.5257896184921265\n",
      "9 900 losses: 0.8854038715362549\n",
      "Epoch 10, Loss: 1.3636080026626587, Accuracy: 61.98796081542969, Test Loss: 3.164858818054199, Test Accuracy: 37.17998123168945\n",
      "10 0 losses: 1.4083846807479858\n",
      "10 100 losses: 1.6276347637176514\n",
      "10 200 losses: 1.5500457286834717\n",
      "10 300 losses: 1.491104245185852\n",
      "10 400 losses: 0.7977654337882996\n",
      "10 500 losses: 1.0147095918655396\n",
      "10 600 losses: 1.1364235877990723\n",
      "10 700 losses: 1.1294976472854614\n",
      "10 800 losses: 1.3650572299957275\n",
      "10 900 losses: 0.8905735015869141\n",
      "Epoch 11, Loss: 1.1812775135040283, Accuracy: 66.72795104980469, Test Loss: 3.5832760334014893, Test Accuracy: 35.839996337890625\n",
      "11 0 losses: 1.2479740381240845\n",
      "11 100 losses: 1.3651094436645508\n",
      "11 200 losses: 1.3569896221160889\n",
      "11 300 losses: 1.036757230758667\n",
      "11 400 losses: 0.8831771612167358\n",
      "11 500 losses: 1.0700068473815918\n",
      "11 600 losses: 0.8020867705345154\n",
      "11 700 losses: 1.2162706851959229\n",
      "11 800 losses: 1.5133708715438843\n",
      "11 900 losses: 0.5954416394233704\n",
      "Epoch 12, Loss: 1.0450009107589722, Accuracy: 70.6039810180664, Test Loss: 3.6510016918182373, Test Accuracy: 37.169979095458984\n",
      "12 0 losses: 0.5384538769721985\n",
      "12 100 losses: 1.1623674631118774\n",
      "12 200 losses: 0.9876484870910645\n",
      "12 300 losses: 1.3744025230407715\n",
      "12 400 losses: 0.5296123027801514\n",
      "12 500 losses: 0.8790966868400574\n",
      "12 600 losses: 0.648521900177002\n",
      "12 700 losses: 0.8065548539161682\n",
      "12 800 losses: 1.2149395942687988\n",
      "12 900 losses: 0.6667606830596924\n",
      "Epoch 13, Loss: 0.8981408476829529, Accuracy: 74.50405883789062, Test Loss: 3.7607691287994385, Test Accuracy: 37.69000244140625\n",
      "13 0 losses: 0.6473191976547241\n",
      "13 100 losses: 1.497631549835205\n",
      "13 200 losses: 1.0428261756896973\n",
      "13 300 losses: 0.8245996236801147\n",
      "13 400 losses: 0.4926239252090454\n",
      "13 500 losses: 0.5888461470603943\n",
      "13 600 losses: 0.9400050640106201\n",
      "13 700 losses: 1.014740228652954\n",
      "13 800 losses: 0.9675769805908203\n",
      "13 900 losses: 0.6063981652259827\n",
      "Epoch 14, Loss: 0.774752140045166, Accuracy: 77.74406433105469, Test Loss: 3.4903154373168945, Test Accuracy: 38.500003814697266\n",
      "14 0 losses: 0.802966296672821\n",
      "14 100 losses: 0.690339982509613\n",
      "14 200 losses: 0.7506313323974609\n",
      "14 300 losses: 1.0359752178192139\n",
      "14 400 losses: 0.6186850666999817\n",
      "14 500 losses: 0.7407869696617126\n",
      "14 600 losses: 0.7446324229240417\n",
      "14 700 losses: 0.9209604859352112\n",
      "14 800 losses: 0.7439537644386292\n",
      "14 900 losses: 0.5319006443023682\n",
      "Epoch 15, Loss: 0.6760334372520447, Accuracy: 80.31214141845703, Test Loss: 3.527486801147461, Test Accuracy: 38.97998809814453\n",
      "15 0 losses: 0.43760979175567627\n",
      "15 100 losses: 0.680586576461792\n",
      "15 200 losses: 0.41297051310539246\n",
      "15 300 losses: 0.641893208026886\n",
      "15 400 losses: 0.4967289865016937\n",
      "15 500 losses: 0.6345089077949524\n",
      "15 600 losses: 0.9281858801841736\n",
      "15 700 losses: 0.44322657585144043\n",
      "15 800 losses: 0.6231955885887146\n",
      "15 900 losses: 0.3534368574619293\n",
      "Epoch 16, Loss: 0.5775195956230164, Accuracy: 82.9681167602539, Test Loss: 3.6801931858062744, Test Accuracy: 38.780006408691406\n",
      "16 0 losses: 0.7099923491477966\n",
      "16 100 losses: 0.8753836154937744\n",
      "16 200 losses: 0.6850883960723877\n",
      "16 300 losses: 0.3200562298297882\n",
      "16 400 losses: 0.3110242784023285\n",
      "16 500 losses: 0.3364078402519226\n",
      "16 600 losses: 0.4533667266368866\n",
      "16 700 losses: 0.44709518551826477\n",
      "16 800 losses: 0.4914624094963074\n",
      "16 900 losses: 0.5892151594161987\n",
      "Epoch 17, Loss: 0.5031886100769043, Accuracy: 85.26408386230469, Test Loss: 3.690448045730591, Test Accuracy: 39.85999298095703\n",
      "17 0 losses: 0.5093357563018799\n",
      "17 100 losses: 0.5623647570610046\n",
      "17 200 losses: 0.6301242709159851\n",
      "17 300 losses: 0.4643579423427582\n",
      "17 400 losses: 0.448984295129776\n",
      "17 500 losses: 0.22769695520401\n",
      "17 600 losses: 0.4503231942653656\n",
      "17 700 losses: 0.42065292596817017\n",
      "17 800 losses: 0.5939822793006897\n",
      "17 900 losses: 0.5247213244438171\n",
      "Epoch 18, Loss: 0.44693779945373535, Accuracy: 86.89018249511719, Test Loss: 3.889084577560425, Test Accuracy: 39.7400016784668\n",
      "18 0 losses: 0.4076538383960724\n",
      "18 100 losses: 0.6166980266571045\n",
      "18 200 losses: 0.396445631980896\n",
      "18 300 losses: 0.43971535563468933\n",
      "18 400 losses: 0.15369322896003723\n",
      "18 500 losses: 0.39275580644607544\n",
      "18 600 losses: 0.5098773837089539\n",
      "18 700 losses: 0.38465654850006104\n",
      "18 800 losses: 0.6436885595321655\n",
      "18 900 losses: 0.17200127243995667\n",
      "Epoch 19, Loss: 0.4005556106567383, Accuracy: 88.15817260742188, Test Loss: 3.8777244091033936, Test Accuracy: 39.49998474121094\n",
      "19 0 losses: 0.3721492886543274\n",
      "19 100 losses: 0.5105956792831421\n",
      "19 200 losses: 0.3305760324001312\n",
      "19 300 losses: 0.2577040493488312\n",
      "19 400 losses: 0.27247464656829834\n",
      "19 500 losses: 0.24952907860279083\n",
      "19 600 losses: 0.21330343186855316\n",
      "19 700 losses: 0.1823907494544983\n",
      "19 800 losses: 0.6213171482086182\n",
      "19 900 losses: 0.28101176023483276\n",
      "Epoch 20, Loss: 0.35786890983581543, Accuracy: 89.46812438964844, Test Loss: 4.112370491027832, Test Accuracy: 40.44998550415039\n",
      "20 0 losses: 0.17702116072177887\n",
      "20 100 losses: 0.5518406629562378\n",
      "20 200 losses: 0.5575628876686096\n",
      "20 300 losses: 0.2771109640598297\n",
      "20 400 losses: 0.18175862729549408\n",
      "20 500 losses: 0.6800501942634583\n",
      "20 600 losses: 0.4865933656692505\n",
      "20 700 losses: 0.3010869026184082\n",
      "20 800 losses: 0.19157135486602783\n",
      "20 900 losses: 0.2149471640586853\n",
      "Epoch 21, Loss: 0.3326185941696167, Accuracy: 90.28218841552734, Test Loss: 4.118165016174316, Test Accuracy: 40.23999786376953\n",
      "21 0 losses: 0.2484312802553177\n",
      "21 100 losses: 0.3172808587551117\n",
      "21 200 losses: 0.2976909577846527\n",
      "21 300 losses: 0.3357098400592804\n",
      "21 400 losses: 0.17638076841831207\n",
      "21 500 losses: 0.3816220462322235\n",
      "21 600 losses: 0.24313583970069885\n",
      "21 700 losses: 0.28450751304626465\n",
      "21 800 losses: 0.4282325506210327\n",
      "21 900 losses: 0.28851553797721863\n",
      "Epoch 22, Loss: 0.3015156686306, Accuracy: 90.94821166992188, Test Loss: 4.02736234664917, Test Accuracy: 40.74998474121094\n",
      "22 0 losses: 0.43838340044021606\n",
      "22 100 losses: 0.21877697110176086\n",
      "22 200 losses: 0.2142411470413208\n",
      "22 300 losses: 0.2411816418170929\n",
      "22 400 losses: 0.28486984968185425\n",
      "22 500 losses: 0.4214038550853729\n",
      "22 600 losses: 0.16389413177967072\n",
      "22 700 losses: 0.267818421125412\n",
      "22 800 losses: 0.22437015175819397\n",
      "22 900 losses: 0.1991024613380432\n",
      "Epoch 23, Loss: 0.275748610496521, Accuracy: 91.7302017211914, Test Loss: 4.219467639923096, Test Accuracy: 39.8599739074707\n",
      "23 0 losses: 0.44952094554901123\n",
      "23 100 losses: 0.3404560983181\n",
      "23 200 losses: 0.27526867389678955\n",
      "23 300 losses: 0.33156734704971313\n",
      "23 400 losses: 0.15841223299503326\n",
      "23 500 losses: 0.19920749962329865\n",
      "23 600 losses: 0.36285164952278137\n",
      "23 700 losses: 0.14652477204799652\n",
      "23 800 losses: 0.03592156246304512\n",
      "23 900 losses: 0.07749830186367035\n",
      "Epoch 24, Loss: 0.2629757225513458, Accuracy: 92.18616485595703, Test Loss: 4.193365573883057, Test Accuracy: 41.549983978271484\n",
      "24 0 losses: 0.2650154232978821\n",
      "24 100 losses: 0.41614922881126404\n",
      "24 200 losses: 0.4402369558811188\n",
      "24 300 losses: 0.24949555099010468\n",
      "24 400 losses: 0.18230481445789337\n",
      "24 500 losses: 0.11643148213624954\n",
      "24 600 losses: 0.40092575550079346\n",
      "24 700 losses: 0.27130836248397827\n",
      "24 800 losses: 0.26651373505592346\n",
      "24 900 losses: 0.22112956643104553\n",
      "Epoch 25, Loss: 0.24669362604618073, Accuracy: 92.65817260742188, Test Loss: 4.323854446411133, Test Accuracy: 41.46997833251953\n",
      "25 0 losses: 0.08705964684486389\n",
      "25 100 losses: 0.3210620582103729\n",
      "25 200 losses: 0.14980334043502808\n",
      "25 300 losses: 0.33122897148132324\n",
      "25 400 losses: 0.38123053312301636\n",
      "25 500 losses: 0.23197071254253387\n",
      "25 600 losses: 0.20860876142978668\n",
      "25 700 losses: 0.28328797221183777\n",
      "25 800 losses: 0.17817983031272888\n",
      "25 900 losses: 0.22841516137123108\n",
      "Epoch 26, Loss: 0.22164295613765717, Accuracy: 93.41617584228516, Test Loss: 4.648566722869873, Test Accuracy: 40.58999252319336\n",
      "26 0 losses: 0.38701578974723816\n",
      "26 100 losses: 0.12759894132614136\n",
      "26 200 losses: 0.07994872331619263\n",
      "26 300 losses: 0.292782187461853\n",
      "26 400 losses: 0.12091858685016632\n",
      "26 500 losses: 0.2408124953508377\n",
      "26 600 losses: 0.10835971683263779\n",
      "26 700 losses: 0.4702620804309845\n",
      "26 800 losses: 0.17140157520771027\n",
      "26 900 losses: 0.2687292993068695\n",
      "Epoch 27, Loss: 0.21712744235992432, Accuracy: 93.53421783447266, Test Loss: 4.302385330200195, Test Accuracy: 40.509979248046875\n",
      "27 0 losses: 0.15456677973270416\n",
      "27 100 losses: 0.25619107484817505\n",
      "27 200 losses: 0.252183198928833\n",
      "27 300 losses: 0.09925764799118042\n",
      "27 400 losses: 0.09225893765687943\n",
      "27 500 losses: 0.26595377922058105\n",
      "27 600 losses: 0.1420791745185852\n",
      "27 700 losses: 0.11037377268075943\n",
      "27 800 losses: 0.3301762640476227\n",
      "27 900 losses: 0.06028824672102928\n",
      "Epoch 28, Loss: 0.20281857252120972, Accuracy: 93.95018005371094, Test Loss: 4.256270408630371, Test Accuracy: 40.50999069213867\n",
      "28 0 losses: 0.27812138199806213\n",
      "28 100 losses: 0.35223713517189026\n",
      "28 200 losses: 0.48301178216934204\n",
      "28 300 losses: 0.11988100409507751\n",
      "28 400 losses: 0.15910854935646057\n",
      "28 500 losses: 0.06603439152240753\n",
      "28 600 losses: 0.3109694719314575\n",
      "28 700 losses: 0.18129564821720123\n",
      "28 800 losses: 0.12170856446027756\n",
      "28 900 losses: 0.025891782715916634\n",
      "Epoch 29, Loss: 0.19474247097969055, Accuracy: 94.12812805175781, Test Loss: 4.512054920196533, Test Accuracy: 40.88996505737305\n",
      "29 0 losses: 0.16689737141132355\n",
      "29 100 losses: 0.34657400846481323\n",
      "29 200 losses: 0.22465206682682037\n",
      "29 300 losses: 0.17332348227500916\n",
      "29 400 losses: 0.33160069584846497\n",
      "29 500 losses: 0.2461075633764267\n",
      "29 600 losses: 0.2189868539571762\n",
      "29 700 losses: 0.1823214739561081\n",
      "29 800 losses: 0.1320415437221527\n",
      "29 900 losses: 0.12805719673633575\n",
      "Epoch 30, Loss: 0.18655666708946228, Accuracy: 94.44418334960938, Test Loss: 4.512763500213623, Test Accuracy: 40.6199836730957\n",
      "30 0 losses: 0.21172934770584106\n",
      "30 100 losses: 0.23754535615444183\n",
      "30 200 losses: 0.15751497447490692\n",
      "30 300 losses: 0.16160255670547485\n",
      "30 400 losses: 0.10676057636737823\n",
      "30 500 losses: 0.25775524973869324\n",
      "30 600 losses: 0.17950664460659027\n",
      "30 700 losses: 0.1665439009666443\n",
      "30 800 losses: 0.05984894931316376\n",
      "30 900 losses: 0.2330310046672821\n",
      "Epoch 31, Loss: 0.1755681037902832, Accuracy: 94.77812957763672, Test Loss: 4.382532596588135, Test Accuracy: 42.199989318847656\n",
      "31 0 losses: 0.2145301252603531\n",
      "31 100 losses: 0.24520576000213623\n",
      "31 200 losses: 0.23060429096221924\n",
      "31 300 losses: 0.13497000932693481\n",
      "31 400 losses: 0.20676471292972565\n",
      "31 500 losses: 0.39013996720314026\n",
      "31 600 losses: 0.20121043920516968\n",
      "31 700 losses: 0.07282797992229462\n",
      "31 800 losses: 0.4010084271430969\n",
      "31 900 losses: 0.27345535159111023\n",
      "Epoch 32, Loss: 0.16787156462669373, Accuracy: 94.98210906982422, Test Loss: 4.421542167663574, Test Accuracy: 40.95998764038086\n",
      "32 0 losses: 0.36754703521728516\n",
      "32 100 losses: 0.12495296448469162\n",
      "32 200 losses: 0.0589912086725235\n",
      "32 300 losses: 0.044501371681690216\n",
      "32 400 losses: 0.04263262823224068\n",
      "32 500 losses: 0.23007844388484955\n",
      "32 600 losses: 0.3211910128593445\n",
      "32 700 losses: 0.13799457252025604\n",
      "32 800 losses: 0.6581915020942688\n",
      "32 900 losses: 0.16611221432685852\n",
      "Epoch 33, Loss: 0.15775975584983826, Accuracy: 95.25213623046875, Test Loss: 4.579061508178711, Test Accuracy: 41.999996185302734\n",
      "33 0 losses: 0.034206822514534\n",
      "33 100 losses: 0.1709495335817337\n",
      "33 200 losses: 0.08537121117115021\n",
      "33 300 losses: 0.35356831550598145\n",
      "33 400 losses: 0.13214637339115143\n",
      "33 500 losses: 0.1963081955909729\n",
      "33 600 losses: 0.1162584200501442\n",
      "33 700 losses: 0.1503412127494812\n",
      "33 800 losses: 0.03739000856876373\n",
      "33 900 losses: 0.020316574722528458\n",
      "Epoch 34, Loss: 0.1513374298810959, Accuracy: 95.42613220214844, Test Loss: 4.963031768798828, Test Accuracy: 41.08997344970703\n",
      "34 0 losses: 0.06465893983840942\n",
      "34 100 losses: 0.22494256496429443\n",
      "34 200 losses: 0.09714960306882858\n",
      "34 300 losses: 0.16647373139858246\n",
      "34 400 losses: 0.14508844912052155\n",
      "34 500 losses: 0.22564995288848877\n",
      "34 600 losses: 0.1349164843559265\n",
      "34 700 losses: 0.24522027373313904\n",
      "34 800 losses: 0.0885181799530983\n",
      "34 900 losses: 0.20384706556797028\n",
      "Epoch 35, Loss: 0.15663395822048187, Accuracy: 95.34011840820312, Test Loss: 4.757523059844971, Test Accuracy: 41.529998779296875\n",
      "35 0 losses: 0.28777968883514404\n",
      "35 100 losses: 0.4037344455718994\n",
      "35 200 losses: 0.05754343420267105\n",
      "35 300 losses: 0.11986654996871948\n",
      "35 400 losses: 0.11429180204868317\n",
      "35 500 losses: 0.07610666006803513\n",
      "35 600 losses: 0.1267680525779724\n",
      "35 700 losses: 0.19652272760868073\n",
      "35 800 losses: 0.07496902346611023\n",
      "35 900 losses: 0.04805947467684746\n",
      "Epoch 36, Loss: 0.13813672959804535, Accuracy: 95.87808990478516, Test Loss: 4.521091938018799, Test Accuracy: 41.63999557495117\n",
      "36 0 losses: 0.12533310055732727\n",
      "36 100 losses: 0.06292825192213058\n",
      "36 200 losses: 0.10874371230602264\n",
      "36 300 losses: 0.17188772559165955\n",
      "36 400 losses: 0.1211024671792984\n",
      "36 500 losses: 0.18434461951255798\n",
      "36 600 losses: 0.32646533846855164\n",
      "36 700 losses: 0.039431773126125336\n",
      "36 800 losses: 0.08354763686656952\n",
      "36 900 losses: 0.09457799792289734\n",
      "Epoch 37, Loss: 0.1398969441652298, Accuracy: 95.78809356689453, Test Loss: 4.267756938934326, Test Accuracy: 41.9799919128418\n",
      "37 0 losses: 0.25641492009162903\n",
      "37 100 losses: 0.14428001642227173\n",
      "37 200 losses: 0.276108980178833\n",
      "37 300 losses: 0.15291839838027954\n",
      "37 400 losses: 0.04659596458077431\n",
      "37 500 losses: 0.10239158570766449\n",
      "37 600 losses: 0.28781014680862427\n",
      "37 700 losses: 0.10264196246862411\n",
      "37 800 losses: 0.1281764656305313\n",
      "37 900 losses: 0.051070183515548706\n",
      "Epoch 38, Loss: 0.13473834097385406, Accuracy: 95.98210906982422, Test Loss: 4.546563625335693, Test Accuracy: 42.23999786376953\n",
      "38 0 losses: 0.04321559891104698\n",
      "38 100 losses: 0.0886782556772232\n",
      "38 200 losses: 0.07849112898111343\n",
      "38 300 losses: 0.08571483939886093\n",
      "38 400 losses: 0.08045194298028946\n",
      "38 500 losses: 0.06882163882255554\n",
      "38 600 losses: 0.37971723079681396\n",
      "38 700 losses: 0.14945480227470398\n",
      "38 800 losses: 0.05838270112872124\n",
      "38 900 losses: 0.03169703856110573\n",
      "Epoch 39, Loss: 0.13213984668254852, Accuracy: 95.90806579589844, Test Loss: 4.369757652282715, Test Accuracy: 41.90998458862305\n",
      "39 0 losses: 0.11209344863891602\n",
      "39 100 losses: 0.05314553156495094\n",
      "39 200 losses: 0.05848449096083641\n",
      "39 300 losses: 0.27209681272506714\n",
      "39 400 losses: 0.08852941542863846\n",
      "39 500 losses: 0.11288288980722427\n",
      "39 600 losses: 0.01153840683400631\n",
      "39 700 losses: 0.030477985739707947\n",
      "39 800 losses: 0.07375961542129517\n",
      "39 900 losses: 0.16343149542808533\n",
      "Epoch 40, Loss: 0.12462330609560013, Accuracy: 96.21405029296875, Test Loss: 4.658308506011963, Test Accuracy: 40.939971923828125\n",
      "40 0 losses: 0.19992800056934357\n",
      "40 100 losses: 0.1336079239845276\n",
      "40 200 losses: 0.03519394248723984\n",
      "40 300 losses: 0.10248589515686035\n",
      "40 400 losses: 0.0587523952126503\n",
      "40 500 losses: 0.09996297210454941\n",
      "40 600 losses: 0.15915857255458832\n",
      "40 700 losses: 0.07771848887205124\n",
      "40 800 losses: 0.18379314243793488\n",
      "40 900 losses: 0.12403944134712219\n",
      "Epoch 41, Loss: 0.1177428737282753, Accuracy: 96.48804473876953, Test Loss: 4.602260112762451, Test Accuracy: 41.33999252319336\n",
      "41 0 losses: 0.04127194359898567\n",
      "41 100 losses: 0.08604099601507187\n",
      "41 200 losses: 0.24960525333881378\n",
      "41 300 losses: 0.19156238436698914\n",
      "41 400 losses: 0.05634235963225365\n",
      "41 500 losses: 0.061605680733919144\n",
      "41 600 losses: 0.12648054957389832\n",
      "41 700 losses: 0.05113042891025543\n",
      "41 800 losses: 0.14293251931667328\n",
      "41 900 losses: 0.1371837556362152\n",
      "Epoch 42, Loss: 0.12105171382427216, Accuracy: 96.3079605102539, Test Loss: 4.720199108123779, Test Accuracy: 41.74998092651367\n",
      "42 0 losses: 0.07350591570138931\n",
      "42 100 losses: 0.059238892048597336\n",
      "42 200 losses: 0.024131670594215393\n",
      "42 300 losses: 0.028566403314471245\n",
      "42 400 losses: 0.0542188361287117\n",
      "42 500 losses: 0.07419583946466446\n",
      "42 600 losses: 0.009558824822306633\n",
      "42 700 losses: 0.08689429610967636\n",
      "42 800 losses: 0.03834471106529236\n",
      "42 900 losses: 0.020809395238757133\n",
      "Epoch 43, Loss: 0.10754453390836716, Accuracy: 96.7559814453125, Test Loss: 4.509352207183838, Test Accuracy: 42.10999298095703\n",
      "43 0 losses: 0.03257616236805916\n",
      "43 100 losses: 0.07484445720911026\n",
      "43 200 losses: 0.14845171570777893\n",
      "43 300 losses: 0.1693764626979828\n",
      "43 400 losses: 0.050024718046188354\n",
      "43 500 losses: 0.02463100478053093\n",
      "43 600 losses: 0.07564814388751984\n",
      "43 700 losses: 0.0711326003074646\n",
      "43 800 losses: 0.11123783141374588\n",
      "43 900 losses: 0.06348387151956558\n",
      "Epoch 44, Loss: 0.11168722808361053, Accuracy: 96.69002532958984, Test Loss: 4.606263160705566, Test Accuracy: 42.159969329833984\n",
      "44 0 losses: 0.17550460994243622\n",
      "44 100 losses: 0.133845254778862\n",
      "44 200 losses: 0.11873040348291397\n",
      "44 300 losses: 0.03786073997616768\n",
      "44 400 losses: 0.1668110489845276\n",
      "44 500 losses: 0.10628136992454529\n",
      "44 600 losses: 0.15560628473758698\n",
      "44 700 losses: 0.12380073219537735\n",
      "44 800 losses: 0.11202512681484222\n",
      "44 900 losses: 0.04483479633927345\n",
      "Epoch 45, Loss: 0.11109920591115952, Accuracy: 96.69602966308594, Test Loss: 4.560104846954346, Test Accuracy: 42.119991302490234\n",
      "45 0 losses: 0.08789315819740295\n",
      "45 100 losses: 0.012713360600173473\n",
      "45 200 losses: 0.030148567631840706\n",
      "45 300 losses: 0.057096514850854874\n",
      "45 400 losses: 0.03772991523146629\n",
      "45 500 losses: 0.04405524209141731\n",
      "45 600 losses: 0.12224865704774857\n",
      "45 700 losses: 0.08311954140663147\n",
      "45 800 losses: 0.013753836043179035\n",
      "45 900 losses: 0.05447660759091377\n",
      "Epoch 46, Loss: 0.10304571688175201, Accuracy: 96.89997100830078, Test Loss: 4.457509517669678, Test Accuracy: 42.70999526977539\n",
      "46 0 losses: 0.24707090854644775\n",
      "46 100 losses: 0.041052285581827164\n",
      "46 200 losses: 0.2570652663707733\n",
      "46 300 losses: 0.13105639815330505\n",
      "46 400 losses: 0.0299591775983572\n",
      "46 500 losses: 0.22756588459014893\n",
      "46 600 losses: 0.10403071343898773\n",
      "46 700 losses: 0.11210045963525772\n",
      "46 800 losses: 0.04538523033261299\n",
      "46 900 losses: 0.04958429932594299\n",
      "Epoch 47, Loss: 0.10389427840709686, Accuracy: 96.8800048828125, Test Loss: 4.583335876464844, Test Accuracy: 41.89997482299805\n",
      "47 0 losses: 0.05434174835681915\n",
      "47 100 losses: 0.047531213611364365\n",
      "47 200 losses: 0.2555294334888458\n",
      "47 300 losses: 0.16988326609134674\n",
      "47 400 losses: 0.053186941891908646\n",
      "47 500 losses: 0.15700310468673706\n",
      "47 600 losses: 0.09942954778671265\n",
      "47 700 losses: 0.03909175097942352\n",
      "47 800 losses: 0.04123044013977051\n",
      "47 900 losses: 0.12171179056167603\n",
      "Epoch 48, Loss: 0.10198710858821869, Accuracy: 96.83805084228516, Test Loss: 4.70989990234375, Test Accuracy: 42.39999008178711\n",
      "48 0 losses: 0.01631246879696846\n",
      "48 100 losses: 0.02656695432960987\n",
      "48 200 losses: 0.04861654341220856\n",
      "48 300 losses: 0.07870950549840927\n",
      "48 400 losses: 0.068053238093853\n",
      "48 500 losses: 0.06620452553033829\n",
      "48 600 losses: 0.06281423568725586\n",
      "48 700 losses: 0.03807138279080391\n",
      "48 800 losses: 0.14741693437099457\n",
      "48 900 losses: 0.0601930245757103\n",
      "Epoch 49, Loss: 0.09662888199090958, Accuracy: 97.20401000976562, Test Loss: 4.649890899658203, Test Accuracy: 41.41999053955078\n",
      "49 0 losses: 0.04085275158286095\n",
      "49 100 losses: 0.11589379608631134\n",
      "49 200 losses: 0.05442181974649429\n",
      "49 300 losses: 0.04701446369290352\n",
      "49 400 losses: 0.1957492083311081\n",
      "49 500 losses: 0.038676995784044266\n",
      "49 600 losses: 0.12849152088165283\n",
      "49 700 losses: 0.061528269201517105\n",
      "49 800 losses: 0.14253999292850494\n",
      "49 900 losses: 0.058357786387205124\n",
      "Epoch 50, Loss: 0.09615045040845871, Accuracy: 97.06207275390625, Test Loss: 4.629011154174805, Test Accuracy: 42.279998779296875\n"
     ]
    }
   ],
   "source": [
    "model = resnet34()\n",
    "model.build(input_shape=(None,32,32,3))\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'_cutout'\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "model.build(input_shape=(None,32,32,3))\n",
    "lr = 1e-4\n",
    "optimizer=optimizers.Adam(lr=lr)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "train_accuracy = tf.keras.metrics.Mean('train_accuracy', dtype=tf.float32)\n",
    "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
    "test_accuracy = tf.keras.metrics.Mean('test_accuracy', dtype=tf.float32)\n",
    "\n",
    "for epoch in range(50):\n",
    "    # if epoch % 5 == 4:\n",
    "    #     lr/=10\n",
    "    #     optimizer.lr = lr\n",
    "    for step,(x,y) in enumerate(train_db):\n",
    "        #这里做一个前向循环,将需要求解梯度放进来\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_onehot=tf.one_hot(y,depth=100)\n",
    "            x, y_onehot = cutout_mask(x,y_onehot)\n",
    "            #[b,32,32,3] => [b,100]\n",
    "            logits=model(x)\n",
    "            #[b] => [b,100]\n",
    "            #compute loss\n",
    "            loss=tf.losses.categorical_crossentropy(y_onehot,logits)\n",
    "    \n",
    "            loss=tf.reduce_mean(loss)\n",
    "            \n",
    "        pred=tf.argmax(logits,axis=1)\n",
    "        pred=tf.cast(pred,dtype=tf.int32)\n",
    "        correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n",
    "        correct=tf.reduce_sum(correct)\n",
    "        acc = correct/x.shape[0]\n",
    "        \n",
    "        train_loss(loss)\n",
    "        train_accuracy(acc)\n",
    "        #计算gradient\n",
    "        grads=tape.gradient(loss,model.trainable_variables)\n",
    "        #传给优化器两个参数：grads和variable，完成梯度更新\n",
    "        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(epoch,step,'losses:',float(loss))\n",
    "            \n",
    "    \n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "        \n",
    "    total_num=0\n",
    "    total_correct=0\n",
    "    for x,y in test_db:\n",
    "        logits=model(x)\n",
    "        y_onehot = tf.one_hot(y,depth = 100)\n",
    "        loss=tf.losses.categorical_crossentropy(y_onehot,logits)\n",
    "        loss=tf.reduce_mean(loss)\n",
    "        \n",
    "        #prob=tf.nn.softmax(logits,axis=1)\n",
    "        pred=tf.argmax(logits,axis=1)\n",
    "        pred=tf.cast(pred,dtype=tf.int32)\n",
    "        correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n",
    "        correct=tf.reduce_sum(correct)\n",
    "\n",
    "        total_num += x.shape[0]\n",
    "        total_correct += int(correct)\n",
    "        \n",
    "        test_accuracy(correct/x.shape[0])\n",
    "        test_loss(loss)\n",
    "    \n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "            \n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                         train_loss.result(), \n",
    "                         train_accuracy.result()*100,\n",
    "                         test_loss.result(), \n",
    "                         test_accuracy.result()*100))\n",
    "\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "model.save_weights('cutout.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 losses: 4.586793899536133\n",
      "0 100 losses: 4.587038040161133\n",
      "0 200 losses: 4.580397129058838\n",
      "0 300 losses: 4.358099460601807\n",
      "0 400 losses: 4.336691856384277\n",
      "0 500 losses: 4.2393341064453125\n",
      "0 600 losses: 4.331840991973877\n",
      "0 700 losses: 4.111297130584717\n",
      "0 800 losses: 4.2459211349487305\n",
      "0 900 losses: 4.081028461456299\n",
      "Epoch 1, Loss: 4.278772830963135, Accuracy: 4.216019153594971, Test Loss: 3.8562304973602295, Test Accuracy: 9.290008544921875\n",
      "1 0 losses: 4.041572570800781\n",
      "1 100 losses: 4.044863224029541\n",
      "1 200 losses: 4.179912567138672\n",
      "1 300 losses: 4.010278701782227\n",
      "1 400 losses: 3.9121639728546143\n",
      "1 500 losses: 4.059662342071533\n",
      "1 600 losses: 3.917426824569702\n",
      "1 700 losses: 3.955514430999756\n",
      "1 800 losses: 3.8790173530578613\n",
      "1 900 losses: 3.381333112716675\n",
      "Epoch 2, Loss: 3.861189126968384, Accuracy: 10.16402816772461, Test Loss: 3.42830753326416, Test Accuracy: 17.740005493164062\n",
      "2 0 losses: 3.735278367996216\n",
      "2 100 losses: 3.760380744934082\n",
      "2 200 losses: 3.722231388092041\n",
      "2 300 losses: 3.606095552444458\n",
      "2 400 losses: 3.655977249145508\n",
      "2 500 losses: 3.824916362762451\n",
      "2 600 losses: 3.6344549655914307\n",
      "2 700 losses: 3.7427759170532227\n",
      "2 800 losses: 3.621957302093506\n",
      "2 900 losses: 2.990365505218506\n",
      "Epoch 3, Loss: 3.5531485080718994, Accuracy: 15.578015327453613, Test Loss: 3.1237924098968506, Test Accuracy: 23.820009231567383\n",
      "3 0 losses: 3.488269567489624\n",
      "3 100 losses: 3.3760290145874023\n",
      "3 200 losses: 3.453122138977051\n",
      "3 300 losses: 3.3803939819335938\n",
      "3 400 losses: 3.3381214141845703\n",
      "3 500 losses: 3.6892197132110596\n",
      "3 600 losses: 3.449779987335205\n",
      "3 700 losses: 3.8693180084228516\n",
      "3 800 losses: 3.5246214866638184\n",
      "3 900 losses: 2.7973101139068604\n",
      "Epoch 4, Loss: 3.2926597595214844, Accuracy: 20.313989639282227, Test Loss: 2.9437344074249268, Test Accuracy: 27.549991607666016\n",
      "4 0 losses: 3.049262285232544\n",
      "4 100 losses: 3.148935556411743\n",
      "4 200 losses: 3.19948673248291\n",
      "4 300 losses: 3.099601984024048\n",
      "4 400 losses: 2.8516130447387695\n",
      "4 500 losses: 3.2937493324279785\n",
      "4 600 losses: 3.0863518714904785\n",
      "4 700 losses: 3.4798238277435303\n",
      "4 800 losses: 3.1874887943267822\n",
      "4 900 losses: 2.738708257675171\n",
      "Epoch 5, Loss: 3.0685060024261475, Accuracy: 24.442018508911133, Test Loss: 2.7793679237365723, Test Accuracy: 31.170001983642578\n",
      "5 0 losses: 2.9960854053497314\n",
      "5 100 losses: 3.1356470584869385\n",
      "5 200 losses: 2.884042263031006\n",
      "5 300 losses: 2.8055922985076904\n",
      "5 400 losses: 2.8905930519104004\n",
      "5 500 losses: 3.195884943008423\n",
      "5 600 losses: 2.707913398742676\n",
      "5 700 losses: 3.083258867263794\n",
      "5 800 losses: 3.195019006729126\n",
      "5 900 losses: 2.341092109680176\n",
      "Epoch 6, Loss: 2.8419671058654785, Accuracy: 29.425981521606445, Test Loss: 2.69529390335083, Test Accuracy: 33.12998962402344\n",
      "6 0 losses: 2.89601469039917\n",
      "6 100 losses: 2.654400110244751\n",
      "6 200 losses: 2.3693675994873047\n",
      "6 300 losses: 2.8529818058013916\n",
      "6 400 losses: 2.8202199935913086\n",
      "6 500 losses: 2.780634880065918\n",
      "6 600 losses: 2.7850894927978516\n",
      "6 700 losses: 2.562706232070923\n",
      "6 800 losses: 3.060500383377075\n",
      "6 900 losses: 2.2458655834198\n",
      "Epoch 7, Loss: 2.615306854248047, Accuracy: 34.73796463012695, Test Loss: 2.655362844467163, Test Accuracy: 34.41999053955078\n",
      "7 0 losses: 2.6496994495391846\n",
      "7 100 losses: 2.4121036529541016\n",
      "7 200 losses: 2.5745928287506104\n",
      "7 300 losses: 2.367281913757324\n",
      "7 400 losses: 2.3219149112701416\n",
      "7 500 losses: 2.4374616146087646\n",
      "7 600 losses: 2.1692867279052734\n",
      "7 700 losses: 2.4576897621154785\n",
      "7 800 losses: 2.6126842498779297\n",
      "7 900 losses: 1.9153268337249756\n",
      "Epoch 8, Loss: 2.3830819129943848, Accuracy: 40.20198440551758, Test Loss: 2.665933132171631, Test Accuracy: 35.02998352050781\n",
      "8 0 losses: 2.201366662979126\n",
      "8 100 losses: 2.287778615951538\n",
      "8 200 losses: 2.2892980575561523\n",
      "8 300 losses: 2.4001526832580566\n",
      "8 400 losses: 2.3042304515838623\n",
      "8 500 losses: 2.1492362022399902\n",
      "8 600 losses: 1.9619231224060059\n",
      "8 700 losses: 2.043276071548462\n",
      "8 800 losses: 2.138413190841675\n",
      "8 900 losses: 1.9747234582901\n",
      "Epoch 9, Loss: 2.1741559505462646, Accuracy: 45.51804733276367, Test Loss: 2.724371910095215, Test Accuracy: 35.79998779296875\n",
      "9 0 losses: 1.9490371942520142\n",
      "9 100 losses: 2.0185670852661133\n",
      "9 200 losses: 1.9194036722183228\n",
      "9 300 losses: 2.2013792991638184\n",
      "9 400 losses: 1.7830613851547241\n",
      "9 500 losses: 2.181593418121338\n",
      "9 600 losses: 2.098687171936035\n",
      "9 700 losses: 2.153594970703125\n",
      "9 800 losses: 2.2487740516662598\n",
      "9 900 losses: 1.6197491884231567\n",
      "Epoch 10, Loss: 1.9929012060165405, Accuracy: 50.80995559692383, Test Loss: 2.794440984725952, Test Accuracy: 35.37998962402344\n",
      "10 0 losses: 1.7374075651168823\n",
      "10 100 losses: 1.7107068300247192\n",
      "10 200 losses: 1.9788742065429688\n",
      "10 300 losses: 2.002178430557251\n",
      "10 400 losses: 1.7483460903167725\n",
      "10 500 losses: 1.7578456401824951\n",
      "10 600 losses: 2.0964879989624023\n",
      "10 700 losses: 2.0975136756896973\n",
      "10 800 losses: 2.2385196685791016\n",
      "10 900 losses: 1.5842801332473755\n",
      "Epoch 11, Loss: 1.838586449623108, Accuracy: 54.961936950683594, Test Loss: 2.8192617893218994, Test Accuracy: 36.939971923828125\n",
      "11 0 losses: 1.489443302154541\n",
      "11 100 losses: 1.8654074668884277\n",
      "11 200 losses: 1.5482783317565918\n",
      "11 300 losses: 1.8824080228805542\n",
      "11 400 losses: 1.5759124755859375\n",
      "11 500 losses: 1.8956936597824097\n",
      "11 600 losses: 1.873754620552063\n",
      "11 700 losses: 1.9020925760269165\n",
      "11 800 losses: 1.8784784078598022\n",
      "11 900 losses: 1.650436282157898\n",
      "Epoch 12, Loss: 1.7003170251846313, Accuracy: 58.57794952392578, Test Loss: 2.8481125831604004, Test Accuracy: 37.54999542236328\n",
      "12 0 losses: 1.5176479816436768\n",
      "12 100 losses: 1.6717220544815063\n",
      "12 200 losses: 1.3350708484649658\n",
      "12 300 losses: 1.8519920110702515\n",
      "12 400 losses: 1.5679593086242676\n",
      "12 500 losses: 1.7167482376098633\n",
      "12 600 losses: 1.5610504150390625\n",
      "12 700 losses: 1.6367599964141846\n",
      "12 800 losses: 1.8419041633605957\n",
      "12 900 losses: 1.543028712272644\n",
      "Epoch 13, Loss: 1.5954387187957764, Accuracy: 61.2459831237793, Test Loss: 2.8358724117279053, Test Accuracy: 37.159969329833984\n",
      "13 0 losses: 1.622446894645691\n",
      "13 100 losses: 1.5481390953063965\n",
      "13 200 losses: 1.4560641050338745\n",
      "13 300 losses: 1.706819772720337\n",
      "13 400 losses: 1.568636178970337\n",
      "13 500 losses: 1.7573809623718262\n",
      "13 600 losses: 1.5696327686309814\n",
      "13 700 losses: 1.7691189050674438\n",
      "13 800 losses: 1.955489158630371\n",
      "13 900 losses: 1.3022537231445312\n",
      "Epoch 14, Loss: 1.5130119323730469, Accuracy: 62.718017578125, Test Loss: 2.930112838745117, Test Accuracy: 36.88999557495117\n",
      "14 0 losses: 1.5265777111053467\n",
      "14 100 losses: 1.6202285289764404\n",
      "14 200 losses: 1.421828269958496\n",
      "14 300 losses: 1.6532390117645264\n",
      "14 400 losses: 1.382135272026062\n",
      "14 500 losses: 1.7081198692321777\n",
      "14 600 losses: 1.476900339126587\n",
      "14 700 losses: 1.4060273170471191\n",
      "14 800 losses: 1.3864740133285522\n",
      "14 900 losses: 1.1704951524734497\n",
      "Epoch 15, Loss: 1.4365084171295166, Accuracy: 64.46192932128906, Test Loss: 2.8471672534942627, Test Accuracy: 37.6199951171875\n",
      "15 0 losses: 1.5448592901229858\n",
      "15 100 losses: 1.3133773803710938\n",
      "15 200 losses: 1.2036365270614624\n",
      "15 300 losses: 1.4323632717132568\n",
      "15 400 losses: 1.1759494543075562\n",
      "15 500 losses: 1.2663904428482056\n",
      "15 600 losses: 1.5392581224441528\n",
      "15 700 losses: 1.2851513624191284\n",
      "15 800 losses: 1.5022163391113281\n",
      "15 900 losses: 1.2706189155578613\n",
      "Epoch 16, Loss: 1.3897695541381836, Accuracy: 65.68595886230469, Test Loss: 2.83951473236084, Test Accuracy: 38.2199821472168\n",
      "16 0 losses: 1.4140945672988892\n",
      "16 100 losses: 1.3658349514007568\n",
      "16 200 losses: 1.5021891593933105\n",
      "16 300 losses: 1.310499906539917\n",
      "16 400 losses: 1.3555740118026733\n",
      "16 500 losses: 1.5012593269348145\n",
      "16 600 losses: 1.2191275358200073\n",
      "16 700 losses: 1.3401867151260376\n",
      "16 800 losses: 1.6296881437301636\n",
      "16 900 losses: 1.5015606880187988\n",
      "Epoch 17, Loss: 1.3317397832870483, Accuracy: 66.5859146118164, Test Loss: 2.7698733806610107, Test Accuracy: 38.86000442504883\n",
      "17 0 losses: 1.1087371110916138\n",
      "17 100 losses: 1.5549216270446777\n",
      "17 200 losses: 1.1811981201171875\n",
      "17 300 losses: 1.2556593418121338\n",
      "17 400 losses: 1.4108140468597412\n",
      "17 500 losses: 1.6653900146484375\n",
      "17 600 losses: 1.2716126441955566\n",
      "17 700 losses: 0.9731616377830505\n",
      "17 800 losses: 1.457434892654419\n",
      "17 900 losses: 1.3965853452682495\n",
      "Epoch 18, Loss: 1.3095769882202148, Accuracy: 67.04197692871094, Test Loss: 2.7318601608276367, Test Accuracy: 39.899993896484375\n",
      "18 0 losses: 1.1815003156661987\n",
      "18 100 losses: 1.3745973110198975\n",
      "18 200 losses: 1.1343969106674194\n",
      "18 300 losses: 1.2873954772949219\n",
      "18 400 losses: 1.2300792932510376\n",
      "18 500 losses: 1.251518726348877\n",
      "18 600 losses: 1.254250168800354\n",
      "18 700 losses: 1.1795998811721802\n",
      "18 800 losses: 1.7046537399291992\n",
      "18 900 losses: 1.3136481046676636\n",
      "Epoch 19, Loss: 1.2731305360794067, Accuracy: 68.1518783569336, Test Loss: 2.7268292903900146, Test Accuracy: 39.219993591308594\n",
      "19 0 losses: 1.1491721868515015\n",
      "19 100 losses: 1.082120418548584\n",
      "19 200 losses: 1.2577611207962036\n",
      "19 300 losses: 1.3851863145828247\n",
      "19 400 losses: 1.1100860834121704\n",
      "19 500 losses: 1.3982386589050293\n",
      "19 600 losses: 1.2043261528015137\n",
      "19 700 losses: 1.2975021600723267\n",
      "19 800 losses: 1.351392388343811\n",
      "19 900 losses: 1.3207000494003296\n",
      "Epoch 20, Loss: 1.250704288482666, Accuracy: 68.4019546508789, Test Loss: 2.7341439723968506, Test Accuracy: 39.079986572265625\n",
      "20 0 losses: 1.2009142637252808\n",
      "20 100 losses: 1.4904407262802124\n",
      "20 200 losses: 1.470647931098938\n",
      "20 300 losses: 1.5078686475753784\n",
      "20 400 losses: 1.2113583087921143\n",
      "20 500 losses: 1.425276756286621\n",
      "20 600 losses: 1.3641427755355835\n",
      "20 700 losses: 1.3442671298980713\n",
      "20 800 losses: 1.2612532377243042\n",
      "20 900 losses: 0.9000413417816162\n",
      "Epoch 21, Loss: 1.2283751964569092, Accuracy: 69.04991149902344, Test Loss: 2.695866346359253, Test Accuracy: 40.37997817993164\n",
      "21 0 losses: 1.4248138666152954\n",
      "21 100 losses: 1.114039659500122\n",
      "21 200 losses: 1.171907901763916\n",
      "21 300 losses: 1.3252975940704346\n",
      "21 400 losses: 1.220226526260376\n",
      "21 500 losses: 1.4709323644638062\n",
      "21 600 losses: 1.0486050844192505\n",
      "21 700 losses: 1.3110049962997437\n",
      "21 800 losses: 1.1353987455368042\n",
      "21 900 losses: 1.23447585105896\n",
      "Epoch 22, Loss: 1.2044607400894165, Accuracy: 69.56991577148438, Test Loss: 2.7148025035858154, Test Accuracy: 39.43998718261719\n",
      "22 0 losses: 1.4299038648605347\n",
      "22 100 losses: 1.3193175792694092\n",
      "22 200 losses: 1.0964038372039795\n",
      "22 300 losses: 1.283632516860962\n",
      "22 400 losses: 0.9948604106903076\n",
      "22 500 losses: 1.3578097820281982\n",
      "22 600 losses: 1.1902722120285034\n",
      "22 700 losses: 1.4798517227172852\n",
      "22 800 losses: 1.4635162353515625\n",
      "22 900 losses: 1.2532000541687012\n",
      "Epoch 23, Loss: 1.1865696907043457, Accuracy: 69.8779067993164, Test Loss: 2.6512694358825684, Test Accuracy: 40.339996337890625\n",
      "23 0 losses: 1.1947500705718994\n",
      "23 100 losses: 1.165633201599121\n",
      "23 200 losses: 1.0763916969299316\n",
      "23 300 losses: 0.9184384346008301\n",
      "23 400 losses: 1.2551758289337158\n",
      "23 500 losses: 1.3323352336883545\n",
      "23 600 losses: 1.1083650588989258\n",
      "23 700 losses: 1.024449348449707\n",
      "23 800 losses: 1.383243441581726\n",
      "23 900 losses: 1.265039324760437\n",
      "Epoch 24, Loss: 1.1669151782989502, Accuracy: 70.27989196777344, Test Loss: 2.691483736038208, Test Accuracy: 40.11998748779297\n",
      "24 0 losses: 1.3624660968780518\n",
      "24 100 losses: 1.2132431268692017\n",
      "24 200 losses: 1.1127598285675049\n",
      "24 300 losses: 0.938331127166748\n",
      "24 400 losses: 1.0109535455703735\n",
      "24 500 losses: 1.1254034042358398\n",
      "24 600 losses: 0.9991589188575745\n",
      "24 700 losses: 1.2975691556930542\n",
      "24 800 losses: 1.0493677854537964\n",
      "24 900 losses: 1.6234004497528076\n",
      "Epoch 25, Loss: 1.154533863067627, Accuracy: 70.48592376708984, Test Loss: 2.6717910766601562, Test Accuracy: 40.369991302490234\n",
      "25 0 losses: 1.3856827020645142\n",
      "25 100 losses: 1.184071660041809\n",
      "25 200 losses: 1.0145690441131592\n",
      "25 300 losses: 1.143246054649353\n",
      "25 400 losses: 1.1770421266555786\n",
      "25 500 losses: 1.1780481338500977\n",
      "25 600 losses: 1.091193437576294\n",
      "25 700 losses: 1.4783090353012085\n",
      "25 800 losses: 1.1179505586624146\n",
      "25 900 losses: 0.9791393876075745\n",
      "Epoch 26, Loss: 1.1443095207214355, Accuracy: 70.79593658447266, Test Loss: 2.6599924564361572, Test Accuracy: 40.509986877441406\n",
      "26 0 losses: 1.050888180732727\n",
      "26 100 losses: 1.0777779817581177\n",
      "26 200 losses: 1.2552300691604614\n",
      "26 300 losses: 1.2636529207229614\n",
      "26 400 losses: 1.2799296379089355\n",
      "26 500 losses: 1.159701943397522\n",
      "26 600 losses: 1.0138705968856812\n",
      "26 700 losses: 1.1752859354019165\n",
      "26 800 losses: 0.9868858456611633\n",
      "26 900 losses: 1.0858540534973145\n",
      "Epoch 27, Loss: 1.1338948011398315, Accuracy: 70.80790710449219, Test Loss: 2.673107862472534, Test Accuracy: 40.62998580932617\n",
      "27 0 losses: 1.324037790298462\n",
      "27 100 losses: 1.18313729763031\n",
      "27 200 losses: 1.0430315732955933\n",
      "27 300 losses: 1.2165095806121826\n",
      "27 400 losses: 0.9841487407684326\n",
      "27 500 losses: 1.0150609016418457\n",
      "27 600 losses: 1.040511131286621\n",
      "27 700 losses: 1.2015044689178467\n",
      "27 800 losses: 1.344443917274475\n",
      "27 900 losses: 0.9444261789321899\n",
      "Epoch 28, Loss: 1.1242074966430664, Accuracy: 71.22392272949219, Test Loss: 2.655143976211548, Test Accuracy: 41.03998565673828\n",
      "28 0 losses: 1.3284246921539307\n",
      "28 100 losses: 1.1469447612762451\n",
      "28 200 losses: 1.144111156463623\n",
      "28 300 losses: 0.9724140763282776\n",
      "28 400 losses: 0.9930456280708313\n",
      "28 500 losses: 1.0152106285095215\n",
      "28 600 losses: 1.1218641996383667\n",
      "28 700 losses: 1.0970609188079834\n",
      "28 800 losses: 1.2600657939910889\n",
      "28 900 losses: 1.0054851770401\n",
      "Epoch 29, Loss: 1.1074403524398804, Accuracy: 71.47798156738281, Test Loss: 2.5891339778900146, Test Accuracy: 41.93998336791992\n",
      "29 0 losses: 1.0172467231750488\n",
      "29 100 losses: 0.8748617768287659\n",
      "29 200 losses: 1.2546955347061157\n",
      "29 300 losses: 1.1470999717712402\n",
      "29 400 losses: 1.0795257091522217\n",
      "29 500 losses: 1.1723053455352783\n",
      "29 600 losses: 1.0124984979629517\n",
      "29 700 losses: 1.5032371282577515\n",
      "29 800 losses: 1.1938042640686035\n",
      "29 900 losses: 0.833869457244873\n",
      "Epoch 30, Loss: 1.104987382888794, Accuracy: 71.76988983154297, Test Loss: 2.5954196453094482, Test Accuracy: 41.069984436035156\n",
      "30 0 losses: 0.9425506591796875\n",
      "30 100 losses: 1.0825108289718628\n",
      "30 200 losses: 1.1887736320495605\n",
      "30 300 losses: 1.0325506925582886\n",
      "30 400 losses: 0.9107173085212708\n",
      "30 500 losses: 0.9192191958427429\n",
      "30 600 losses: 1.0756877660751343\n",
      "30 700 losses: 1.0197521448135376\n",
      "30 800 losses: 1.0305581092834473\n",
      "30 900 losses: 0.9971059560775757\n",
      "Epoch 31, Loss: 1.0915277004241943, Accuracy: 71.92790222167969, Test Loss: 2.6796271800994873, Test Accuracy: 40.729984283447266\n",
      "31 0 losses: 1.2243789434432983\n",
      "31 100 losses: 1.0485846996307373\n",
      "31 200 losses: 1.2585927248001099\n",
      "31 300 losses: 1.0055499076843262\n",
      "31 400 losses: 1.2622662782669067\n",
      "31 500 losses: 0.9034246802330017\n",
      "31 600 losses: 1.2397743463516235\n",
      "31 700 losses: 1.2126164436340332\n",
      "31 800 losses: 1.1040254831314087\n",
      "31 900 losses: 1.1201571226119995\n",
      "Epoch 32, Loss: 1.092848300933838, Accuracy: 71.82389831542969, Test Loss: 2.607954502105713, Test Accuracy: 41.84998321533203\n",
      "32 0 losses: 0.9956567287445068\n",
      "32 100 losses: 0.995270848274231\n",
      "32 200 losses: 1.229491114616394\n",
      "32 300 losses: 1.1091089248657227\n",
      "32 400 losses: 1.1935077905654907\n",
      "32 500 losses: 1.217322826385498\n",
      "32 600 losses: 1.2948944568634033\n",
      "32 700 losses: 0.9779800176620483\n",
      "32 800 losses: 1.2557373046875\n",
      "32 900 losses: 1.1735862493515015\n",
      "Epoch 33, Loss: 1.0829716920852661, Accuracy: 72.28395080566406, Test Loss: 2.5903091430664062, Test Accuracy: 41.67998504638672\n",
      "33 0 losses: 0.9897105693817139\n",
      "33 100 losses: 0.9851329922676086\n",
      "33 200 losses: 1.0040558576583862\n",
      "33 300 losses: 1.0644623041152954\n",
      "33 400 losses: 0.8585795760154724\n",
      "33 500 losses: 1.039892315864563\n",
      "33 600 losses: 0.7576667070388794\n",
      "33 700 losses: 0.9983229041099548\n",
      "33 800 losses: 1.3213304281234741\n",
      "33 900 losses: 1.1111172437667847\n",
      "Epoch 34, Loss: 1.071808934211731, Accuracy: 71.93588256835938, Test Loss: 2.632089614868164, Test Accuracy: 41.150001525878906\n",
      "34 0 losses: 1.0049816370010376\n",
      "34 100 losses: 0.8779715895652771\n",
      "34 200 losses: 1.0891826152801514\n",
      "34 300 losses: 1.1458518505096436\n",
      "34 400 losses: 1.0836750268936157\n",
      "34 500 losses: 1.0763745307922363\n",
      "34 600 losses: 1.2241170406341553\n",
      "34 700 losses: 1.0149056911468506\n",
      "34 800 losses: 1.3184897899627686\n",
      "34 900 losses: 0.9318780303001404\n",
      "Epoch 35, Loss: 1.072480320930481, Accuracy: 71.88790893554688, Test Loss: 2.6309518814086914, Test Accuracy: 41.3599853515625\n",
      "35 0 losses: 1.2163585424423218\n",
      "35 100 losses: 0.7656764388084412\n",
      "35 200 losses: 1.0684443712234497\n",
      "35 300 losses: 1.1615612506866455\n",
      "35 400 losses: 1.1791906356811523\n",
      "35 500 losses: 1.1010266542434692\n",
      "35 600 losses: 0.9579994082450867\n",
      "35 700 losses: 1.0927380323410034\n",
      "35 800 losses: 1.034001111984253\n",
      "35 900 losses: 0.951951265335083\n",
      "Epoch 36, Loss: 1.0610127449035645, Accuracy: 72.1199722290039, Test Loss: 2.6535091400146484, Test Accuracy: 40.879981994628906\n",
      "36 0 losses: 1.0489271879196167\n",
      "36 100 losses: 0.8714445233345032\n",
      "36 200 losses: 0.9482042789459229\n",
      "36 300 losses: 1.1737996339797974\n",
      "36 400 losses: 1.1879146099090576\n",
      "36 500 losses: 0.8122909069061279\n",
      "36 600 losses: 1.2925539016723633\n",
      "36 700 losses: 0.879042387008667\n",
      "36 800 losses: 1.2866305112838745\n",
      "36 900 losses: 1.0308641195297241\n",
      "Epoch 37, Loss: 1.0629898309707642, Accuracy: 72.2138671875, Test Loss: 2.6546072959899902, Test Accuracy: 41.69999313354492\n",
      "37 0 losses: 1.0555310249328613\n",
      "37 100 losses: 0.9000254273414612\n",
      "37 200 losses: 1.119960069656372\n",
      "37 300 losses: 1.1544855833053589\n",
      "37 400 losses: 0.8439459204673767\n",
      "37 500 losses: 0.9720385074615479\n",
      "37 600 losses: 1.0583118200302124\n",
      "37 700 losses: 1.2152607440948486\n",
      "37 800 losses: 1.0213240385055542\n",
      "37 900 losses: 0.8248783946037292\n",
      "Epoch 38, Loss: 1.0517276525497437, Accuracy: 72.64387512207031, Test Loss: 2.6212072372436523, Test Accuracy: 41.57999801635742\n",
      "38 0 losses: 1.1157432794570923\n",
      "38 100 losses: 1.219763159751892\n",
      "38 200 losses: 1.0532019138336182\n",
      "38 300 losses: 1.109066128730774\n",
      "38 400 losses: 0.96197110414505\n",
      "38 500 losses: 1.0323712825775146\n",
      "38 600 losses: 0.8289539217948914\n",
      "38 700 losses: 1.0399158000946045\n",
      "38 800 losses: 1.0235902070999146\n",
      "38 900 losses: 1.0464633703231812\n",
      "Epoch 39, Loss: 1.0477428436279297, Accuracy: 72.6119613647461, Test Loss: 2.607090473175049, Test Accuracy: 41.829994201660156\n",
      "39 0 losses: 1.0516008138656616\n",
      "39 100 losses: 1.0136492252349854\n",
      "39 200 losses: 1.1135525703430176\n",
      "39 300 losses: 0.9289063811302185\n",
      "39 400 losses: 0.8479303121566772\n",
      "39 500 losses: 0.9788722395896912\n",
      "39 600 losses: 1.1298249959945679\n",
      "39 700 losses: 1.1507718563079834\n",
      "39 800 losses: 0.9358587861061096\n",
      "39 900 losses: 1.3753900527954102\n",
      "Epoch 40, Loss: 1.0445550680160522, Accuracy: 72.71197509765625, Test Loss: 2.6123971939086914, Test Accuracy: 41.04000473022461\n",
      "40 0 losses: 1.036840796470642\n",
      "40 100 losses: 0.8499312400817871\n",
      "40 200 losses: 1.04990553855896\n",
      "40 300 losses: 0.9636015892028809\n",
      "40 400 losses: 0.916945219039917\n",
      "40 500 losses: 0.930057168006897\n",
      "40 600 losses: 1.1136195659637451\n",
      "40 700 losses: 1.0591917037963867\n",
      "40 800 losses: 0.9561023116111755\n",
      "40 900 losses: 0.889018177986145\n",
      "Epoch 41, Loss: 1.0387400388717651, Accuracy: 72.9359359741211, Test Loss: 2.5808444023132324, Test Accuracy: 42.12998580932617\n",
      "41 0 losses: 1.071192979812622\n",
      "41 100 losses: 1.0201983451843262\n",
      "41 200 losses: 1.2554185390472412\n",
      "41 300 losses: 1.0296841859817505\n",
      "41 400 losses: 1.0913745164871216\n",
      "41 500 losses: 1.0042145252227783\n",
      "41 600 losses: 1.222008466720581\n",
      "41 700 losses: 0.8879930973052979\n",
      "41 800 losses: 1.1841319799423218\n",
      "41 900 losses: 0.9543635845184326\n",
      "Epoch 42, Loss: 1.0392965078353882, Accuracy: 72.77194213867188, Test Loss: 2.6002414226531982, Test Accuracy: 42.26999282836914\n",
      "42 0 losses: 0.971646785736084\n",
      "42 100 losses: 1.0195270776748657\n",
      "42 200 losses: 0.8484737277030945\n",
      "42 300 losses: 1.045408010482788\n",
      "42 400 losses: 1.0874742269515991\n",
      "42 500 losses: 0.9932724833488464\n",
      "42 600 losses: 0.9576979875564575\n",
      "42 700 losses: 1.2947430610656738\n",
      "42 800 losses: 1.0697391033172607\n",
      "42 900 losses: 1.1855370998382568\n",
      "Epoch 43, Loss: 1.0271426439285278, Accuracy: 73.25994110107422, Test Loss: 2.659773349761963, Test Accuracy: 41.8799934387207\n",
      "43 0 losses: 0.999410092830658\n",
      "43 100 losses: 0.921478807926178\n",
      "43 200 losses: 0.9279986619949341\n",
      "43 300 losses: 0.7958331108093262\n",
      "43 400 losses: 1.0971091985702515\n",
      "43 500 losses: 0.9767366051673889\n",
      "43 600 losses: 1.0521183013916016\n",
      "43 700 losses: 0.9855188727378845\n",
      "43 800 losses: 1.269775390625\n",
      "43 900 losses: 1.001857876777649\n",
      "Epoch 44, Loss: 1.0188090801239014, Accuracy: 72.95195770263672, Test Loss: 2.603463649749756, Test Accuracy: 42.20999526977539\n",
      "44 0 losses: 1.3089805841445923\n",
      "44 100 losses: 0.9793325662612915\n",
      "44 200 losses: 1.033483862876892\n",
      "44 300 losses: 1.099372148513794\n",
      "44 400 losses: 0.8926292657852173\n",
      "44 500 losses: 1.0349578857421875\n",
      "44 600 losses: 0.8004110455513\n",
      "44 700 losses: 0.9381160736083984\n",
      "44 800 losses: 1.2149314880371094\n",
      "44 900 losses: 1.2201248407363892\n",
      "Epoch 45, Loss: 1.0194830894470215, Accuracy: 73.00796508789062, Test Loss: 2.5634899139404297, Test Accuracy: 42.320003509521484\n",
      "45 0 losses: 0.9773014187812805\n",
      "45 100 losses: 1.0546940565109253\n",
      "45 200 losses: 1.0491536855697632\n",
      "45 300 losses: 1.0948947668075562\n",
      "45 400 losses: 0.9815028309822083\n",
      "45 500 losses: 1.052564263343811\n",
      "45 600 losses: 0.9227997064590454\n",
      "45 700 losses: 0.9381040334701538\n",
      "45 800 losses: 1.0006513595581055\n",
      "45 900 losses: 1.0701411962509155\n",
      "Epoch 46, Loss: 1.0177487134933472, Accuracy: 72.93797302246094, Test Loss: 2.5450687408447266, Test Accuracy: 42.78998565673828\n",
      "46 0 losses: 1.1265361309051514\n",
      "46 100 losses: 1.2190667390823364\n",
      "46 200 losses: 1.2157009840011597\n",
      "46 300 losses: 1.169409155845642\n",
      "46 400 losses: 1.0195667743682861\n",
      "46 500 losses: 1.0585328340530396\n",
      "46 600 losses: 1.310514211654663\n",
      "46 700 losses: 0.8944238424301147\n",
      "46 800 losses: 0.9013917446136475\n",
      "46 900 losses: 0.8023030757904053\n",
      "Epoch 47, Loss: 1.0117897987365723, Accuracy: 73.37599182128906, Test Loss: 2.55731463432312, Test Accuracy: 42.55998992919922\n",
      "47 0 losses: 0.8815443515777588\n",
      "47 100 losses: 1.2288285493850708\n",
      "47 200 losses: 0.8384873867034912\n",
      "47 300 losses: 0.9802969098091125\n",
      "47 400 losses: 1.0055726766586304\n",
      "47 500 losses: 1.038856029510498\n",
      "47 600 losses: 1.0542279481887817\n",
      "47 700 losses: 0.9234166145324707\n",
      "47 800 losses: 0.9548451900482178\n",
      "47 900 losses: 0.9512699842453003\n",
      "Epoch 48, Loss: 1.0046921968460083, Accuracy: 73.4399642944336, Test Loss: 2.5997684001922607, Test Accuracy: 42.17997741699219\n",
      "48 0 losses: 1.011178731918335\n",
      "48 100 losses: 0.8766142129898071\n",
      "48 200 losses: 1.0662319660186768\n",
      "48 300 losses: 0.8370368480682373\n",
      "48 400 losses: 1.0475083589553833\n",
      "48 500 losses: 0.9431636333465576\n",
      "48 600 losses: 1.1339142322540283\n",
      "48 700 losses: 0.9823837280273438\n",
      "48 800 losses: 1.237585186958313\n",
      "48 900 losses: 0.8776468634605408\n",
      "Epoch 49, Loss: 1.0082205533981323, Accuracy: 73.1939468383789, Test Loss: 2.5605874061584473, Test Accuracy: 42.74000549316406\n",
      "49 0 losses: 0.9019010066986084\n",
      "49 100 losses: 1.2539985179901123\n",
      "49 200 losses: 1.012161374092102\n",
      "49 300 losses: 0.9841442704200745\n",
      "49 400 losses: 0.9170820713043213\n",
      "49 500 losses: 0.928110659122467\n",
      "49 600 losses: 0.95850670337677\n",
      "49 700 losses: 0.9895437359809875\n",
      "49 800 losses: 1.1352133750915527\n",
      "49 900 losses: 0.9439898729324341\n",
      "Epoch 50, Loss: 1.0090835094451904, Accuracy: 73.4039535522461, Test Loss: 2.607755661010742, Test Accuracy: 42.67998123168945\n"
     ]
    }
   ],
   "source": [
    "model = resnet34()\n",
    "model.build(input_shape=(None,32,32,3))\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'_mixup'\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "model.build(input_shape=(None,32,32,3))\n",
    "lr = 1e-4\n",
    "optimizer=optimizers.Adam(lr=lr)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "train_accuracy = tf.keras.metrics.Mean('train_accuracy', dtype=tf.float32)\n",
    "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
    "test_accuracy = tf.keras.metrics.Mean('test_accuracy', dtype=tf.float32)\n",
    "\n",
    "for epoch in range(50):\n",
    "    # if epoch % 5 == 4:\n",
    "    #     lr/=10\n",
    "    #     optimizer.lr = lr\n",
    "    for step,(x,y) in enumerate(train_db):\n",
    "        #这里做一个前向循环,将需要求解梯度放进来\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_onehot=tf.one_hot(y,depth=100)\n",
    "            x, y_onehot = mixup_mask(x,y_onehot)\n",
    "            #[b,32,32,3] => [b,100]\n",
    "            logits=model(x)\n",
    "            #[b] => [b,100]\n",
    "            #compute loss\n",
    "            loss=tf.losses.categorical_crossentropy(y_onehot,logits)\n",
    "    \n",
    "            loss=tf.reduce_mean(loss)\n",
    "            \n",
    "        pred=tf.argmax(logits,axis=1)\n",
    "        pred=tf.cast(pred,dtype=tf.int32)\n",
    "        correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n",
    "        correct=tf.reduce_sum(correct)\n",
    "        acc = correct/x.shape[0]\n",
    "        \n",
    "        train_loss(loss)\n",
    "        train_accuracy(acc)\n",
    "        #计算gradient\n",
    "        grads=tape.gradient(loss,model.trainable_variables)\n",
    "        #传给优化器两个参数：grads和variable，完成梯度更新\n",
    "        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(epoch,step,'losses:',float(loss))\n",
    "            \n",
    "    \n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "        \n",
    "    total_num=0\n",
    "    total_correct=0\n",
    "    for x,y in test_db:\n",
    "        logits=model(x)\n",
    "        y_onehot = tf.one_hot(y,depth = 100)\n",
    "        loss=tf.losses.categorical_crossentropy(y_onehot,logits)\n",
    "        loss=tf.reduce_mean(loss)\n",
    "        \n",
    "        #prob=tf.nn.softmax(logits,axis=1)\n",
    "        pred=tf.argmax(logits,axis=1)\n",
    "        pred=tf.cast(pred,dtype=tf.int32)\n",
    "        correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n",
    "        correct=tf.reduce_sum(correct)\n",
    "\n",
    "        total_num += x.shape[0]\n",
    "        total_correct += int(correct)\n",
    "        \n",
    "        test_accuracy(correct/x.shape[0])\n",
    "        test_loss(loss)\n",
    "    \n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "            \n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                         train_loss.result(), \n",
    "                         train_accuracy.result()*100,\n",
    "                         test_loss.result(), \n",
    "                         test_accuracy.result()*100))\n",
    "\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "model.save_weights('mixup.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 losses: 4.623771667480469\n",
      "0 100 losses: 4.638014316558838\n",
      "0 200 losses: 4.54831600189209\n",
      "0 300 losses: 4.331723690032959\n",
      "0 400 losses: 4.3754191398620605\n",
      "0 500 losses: 4.244229316711426\n",
      "0 600 losses: 4.376081466674805\n",
      "0 700 losses: 4.142012596130371\n",
      "0 800 losses: 4.1289520263671875\n",
      "0 900 losses: 3.907952308654785\n",
      "Epoch 1, Loss: 4.249817848205566, Accuracy: 4.762017726898193, Test Loss: 3.8200714588165283, Test Accuracy: 9.48000717163086\n",
      "1 0 losses: 4.173576354980469\n",
      "1 100 losses: 4.044555187225342\n",
      "1 200 losses: 4.122124195098877\n",
      "1 300 losses: 3.9464609622955322\n",
      "1 400 losses: 3.7651538848876953\n",
      "1 500 losses: 4.017367362976074\n",
      "1 600 losses: 3.972993850708008\n",
      "1 700 losses: 3.997004985809326\n",
      "1 800 losses: 3.854299306869507\n",
      "1 900 losses: 3.37711763381958\n",
      "Epoch 2, Loss: 3.8290576934814453, Accuracy: 12.252025604248047, Test Loss: 3.381711483001709, Test Accuracy: 18.3900089263916\n",
      "2 0 losses: 3.6837966442108154\n",
      "2 100 losses: 3.8258373737335205\n",
      "2 200 losses: 3.870396375656128\n",
      "2 300 losses: 3.483006238937378\n",
      "2 400 losses: 3.466893196105957\n",
      "2 500 losses: 3.852835178375244\n",
      "2 600 losses: 3.7070975303649902\n",
      "2 700 losses: 3.8661348819732666\n",
      "2 800 losses: 3.466892719268799\n",
      "2 900 losses: 3.0496273040771484\n",
      "Epoch 3, Loss: 3.5205953121185303, Accuracy: 18.654001235961914, Test Loss: 3.0956015586853027, Test Accuracy: 24.0000057220459\n",
      "3 0 losses: 3.547595500946045\n",
      "3 100 losses: 3.4147887229919434\n",
      "3 200 losses: 3.563601493835449\n",
      "3 300 losses: 3.456050157546997\n",
      "3 400 losses: 3.122943162918091\n",
      "3 500 losses: 3.637885093688965\n",
      "3 600 losses: 3.3990163803100586\n",
      "3 700 losses: 3.474923610687256\n",
      "3 800 losses: 3.2909984588623047\n",
      "3 900 losses: 2.76652193069458\n",
      "Epoch 4, Loss: 3.2655365467071533, Accuracy: 24.27600860595703, Test Loss: 2.8786368370056152, Test Accuracy: 28.31999969482422\n",
      "4 0 losses: 3.316249370574951\n",
      "4 100 losses: 3.2285661697387695\n",
      "4 200 losses: 3.149155378341675\n",
      "4 300 losses: 3.291841506958008\n",
      "4 400 losses: 2.8139607906341553\n",
      "4 500 losses: 3.3133585453033447\n",
      "4 600 losses: 3.0381293296813965\n",
      "4 700 losses: 3.233020544052124\n",
      "4 800 losses: 3.205483913421631\n",
      "4 900 losses: 2.584688186645508\n",
      "Epoch 5, Loss: 3.032491445541382, Accuracy: 29.63201332092285, Test Loss: 2.7032766342163086, Test Accuracy: 32.2599983215332\n",
      "5 0 losses: 3.146125555038452\n",
      "5 100 losses: 2.928558111190796\n",
      "5 200 losses: 2.9656167030334473\n",
      "5 300 losses: 2.963850736618042\n",
      "5 400 losses: 2.818939208984375\n",
      "5 500 losses: 3.1719560623168945\n",
      "5 600 losses: 2.7479867935180664\n",
      "5 700 losses: 3.0860073566436768\n",
      "5 800 losses: 2.825704574584961\n",
      "5 900 losses: 2.548785448074341\n",
      "Epoch 6, Loss: 2.8136045932769775, Accuracy: 35.19598388671875, Test Loss: 2.566934585571289, Test Accuracy: 35.449981689453125\n",
      "6 0 losses: 2.7940008640289307\n",
      "6 100 losses: 2.7385621070861816\n",
      "6 200 losses: 2.965481758117676\n",
      "6 300 losses: 2.877063274383545\n",
      "6 400 losses: 2.650022506713867\n",
      "6 500 losses: 2.966764450073242\n",
      "6 600 losses: 2.5896854400634766\n",
      "6 700 losses: 2.664895534515381\n",
      "6 800 losses: 2.643298864364624\n",
      "6 900 losses: 2.3478755950927734\n",
      "Epoch 7, Loss: 2.6027462482452393, Accuracy: 40.239990234375, Test Loss: 2.4965004920959473, Test Accuracy: 37.699989318847656\n",
      "7 0 losses: 2.7731707096099854\n",
      "7 100 losses: 2.609564781188965\n",
      "7 200 losses: 2.623260736465454\n",
      "7 300 losses: 2.4804747104644775\n",
      "7 400 losses: 2.2680296897888184\n",
      "7 500 losses: 2.6068148612976074\n",
      "7 600 losses: 2.368288516998291\n",
      "7 700 losses: 2.679898738861084\n",
      "7 800 losses: 2.408513069152832\n",
      "7 900 losses: 2.0219876766204834\n",
      "Epoch 8, Loss: 2.402920722961426, Accuracy: 45.96001434326172, Test Loss: 2.549731731414795, Test Accuracy: 37.67000198364258\n",
      "8 0 losses: 2.5830979347229004\n",
      "8 100 losses: 2.3725316524505615\n",
      "8 200 losses: 2.420577049255371\n",
      "8 300 losses: 2.33154296875\n",
      "8 400 losses: 2.0819358825683594\n",
      "8 500 losses: 2.465838670730591\n",
      "8 600 losses: 2.139237642288208\n",
      "8 700 losses: 2.4590463638305664\n",
      "8 800 losses: 2.3133955001831055\n",
      "8 900 losses: 1.8103113174438477\n",
      "Epoch 9, Loss: 2.224774122238159, Accuracy: 50.85596466064453, Test Loss: 2.558823347091675, Test Accuracy: 38.199989318847656\n",
      "9 0 losses: 2.3857955932617188\n",
      "9 100 losses: 2.1756420135498047\n",
      "9 200 losses: 2.310760259628296\n",
      "9 300 losses: 2.1449034214019775\n",
      "9 400 losses: 2.1827425956726074\n",
      "9 500 losses: 1.9805026054382324\n",
      "9 600 losses: 2.1102347373962402\n",
      "9 700 losses: 2.1358888149261475\n",
      "9 800 losses: 2.151766300201416\n",
      "9 900 losses: 1.610252022743225\n",
      "Epoch 10, Loss: 2.064239501953125, Accuracy: 55.5959358215332, Test Loss: 2.5459909439086914, Test Accuracy: 38.9999885559082\n",
      "10 0 losses: 2.074279308319092\n",
      "10 100 losses: 2.073781728744507\n",
      "10 200 losses: 2.1854007244110107\n",
      "10 300 losses: 2.120210886001587\n",
      "10 400 losses: 1.8456615209579468\n",
      "10 500 losses: 2.1292200088500977\n",
      "10 600 losses: 2.0393667221069336\n",
      "10 700 losses: 1.8867886066436768\n",
      "10 800 losses: 1.9116460084915161\n",
      "10 900 losses: 1.4011073112487793\n",
      "Epoch 11, Loss: 1.9365713596343994, Accuracy: 59.73597717285156, Test Loss: 2.5462372303009033, Test Accuracy: 38.71998977661133\n",
      "11 0 losses: 2.1265370845794678\n",
      "11 100 losses: 1.8430317640304565\n",
      "11 200 losses: 2.008131504058838\n",
      "11 300 losses: 1.9258317947387695\n",
      "11 400 losses: 1.5483648777008057\n",
      "11 500 losses: 1.859062910079956\n",
      "11 600 losses: 1.8107235431671143\n",
      "11 700 losses: 1.899240255355835\n",
      "11 800 losses: 1.741339087486267\n",
      "11 900 losses: 1.365505576133728\n",
      "Epoch 12, Loss: 1.8089321851730347, Accuracy: 63.52791213989258, Test Loss: 2.7000460624694824, Test Accuracy: 39.2599983215332\n",
      "12 0 losses: 1.844857931137085\n",
      "12 100 losses: 1.7586365938186646\n",
      "12 200 losses: 1.9617327451705933\n",
      "12 300 losses: 1.8452279567718506\n",
      "12 400 losses: 1.4982872009277344\n",
      "12 500 losses: 1.8198856115341187\n",
      "12 600 losses: 1.7392703294754028\n",
      "12 700 losses: 1.930946707725525\n",
      "12 800 losses: 1.836782693862915\n",
      "12 900 losses: 1.356207013130188\n",
      "Epoch 13, Loss: 1.7172527313232422, Accuracy: 66.23393249511719, Test Loss: 2.6534597873687744, Test Accuracy: 40.41998291015625\n",
      "13 0 losses: 1.9360096454620361\n",
      "13 100 losses: 1.8197112083435059\n",
      "13 200 losses: 1.6128262281417847\n",
      "13 300 losses: 1.7310622930526733\n",
      "13 400 losses: 1.3897165060043335\n",
      "13 500 losses: 1.7447580099105835\n",
      "13 600 losses: 1.4672013521194458\n",
      "13 700 losses: 1.843621850013733\n",
      "13 800 losses: 1.739050269126892\n",
      "13 900 losses: 1.5758029222488403\n",
      "Epoch 14, Loss: 1.610690951347351, Accuracy: 69.70791625976562, Test Loss: 2.681380033493042, Test Accuracy: 40.49000549316406\n",
      "14 0 losses: 1.5973092317581177\n",
      "14 100 losses: 1.7076382637023926\n",
      "14 200 losses: 1.5205798149108887\n",
      "14 300 losses: 1.7333807945251465\n",
      "14 400 losses: 1.2306857109069824\n",
      "14 500 losses: 1.6448522806167603\n",
      "14 600 losses: 1.4251669645309448\n",
      "14 700 losses: 1.679459571838379\n",
      "14 800 losses: 1.6931953430175781\n",
      "14 900 losses: 1.2510544061660767\n",
      "Epoch 15, Loss: 1.536574363708496, Accuracy: 71.86598205566406, Test Loss: 2.6740524768829346, Test Accuracy: 41.07999801635742\n",
      "15 0 losses: 1.589390516281128\n",
      "15 100 losses: 1.6598107814788818\n",
      "15 200 losses: 1.4577075242996216\n",
      "15 300 losses: 1.5254939794540405\n",
      "15 400 losses: 1.2127714157104492\n",
      "15 500 losses: 1.3211833238601685\n",
      "15 600 losses: 1.4244707822799683\n",
      "15 700 losses: 1.6660690307617188\n",
      "15 800 losses: 1.6974117755889893\n",
      "15 900 losses: 1.2135406732559204\n",
      "Epoch 16, Loss: 1.464950680732727, Accuracy: 74.49005889892578, Test Loss: 2.7131242752075195, Test Accuracy: 41.22999954223633\n",
      "16 0 losses: 1.5236834287643433\n",
      "16 100 losses: 1.512782335281372\n",
      "16 200 losses: 1.504723072052002\n",
      "16 300 losses: 1.6846853494644165\n",
      "16 400 losses: 1.1096973419189453\n",
      "16 500 losses: 1.4527802467346191\n",
      "16 600 losses: 1.3244534730911255\n",
      "16 700 losses: 1.5747268199920654\n",
      "16 800 losses: 1.562281847000122\n",
      "16 900 losses: 1.1878899335861206\n",
      "Epoch 17, Loss: 1.408508539199829, Accuracy: 75.8420639038086, Test Loss: 2.7788009643554688, Test Accuracy: 40.62998962402344\n",
      "17 0 losses: 1.376692771911621\n",
      "17 100 losses: 1.5276520252227783\n",
      "17 200 losses: 1.3152525424957275\n",
      "17 300 losses: 1.5083853006362915\n",
      "17 400 losses: 1.1910674571990967\n",
      "17 500 losses: 1.267278790473938\n",
      "17 600 losses: 1.1581394672393799\n",
      "17 700 losses: 1.34828519821167\n",
      "17 800 losses: 1.357635498046875\n",
      "17 900 losses: 1.1883978843688965\n",
      "Epoch 18, Loss: 1.359634518623352, Accuracy: 77.35807037353516, Test Loss: 2.7247555255889893, Test Accuracy: 41.019981384277344\n",
      "18 0 losses: 1.3474453687667847\n",
      "18 100 losses: 1.4530997276306152\n",
      "18 200 losses: 1.2789372205734253\n",
      "18 300 losses: 1.4506219625473022\n",
      "18 400 losses: 1.3532577753067017\n",
      "18 500 losses: 1.411552906036377\n",
      "18 600 losses: 1.4032498598098755\n",
      "18 700 losses: 1.456959843635559\n",
      "18 800 losses: 1.2031128406524658\n",
      "18 900 losses: 1.3438398838043213\n",
      "Epoch 19, Loss: 1.3112425804138184, Accuracy: 78.90617370605469, Test Loss: 2.8096964359283447, Test Accuracy: 41.739990234375\n",
      "19 0 losses: 1.313464641571045\n",
      "19 100 losses: 1.3711673021316528\n",
      "19 200 losses: 1.3196265697479248\n",
      "19 300 losses: 1.4948195219039917\n",
      "19 400 losses: 1.268182396888733\n",
      "19 500 losses: 1.266778826713562\n",
      "19 600 losses: 1.206831693649292\n",
      "19 700 losses: 1.1951580047607422\n",
      "19 800 losses: 1.3791329860687256\n",
      "19 900 losses: 1.0927153825759888\n",
      "Epoch 20, Loss: 1.2716771364212036, Accuracy: 80.27415466308594, Test Loss: 2.6873810291290283, Test Accuracy: 42.459983825683594\n",
      "20 0 losses: 1.2488969564437866\n",
      "20 100 losses: 1.422764539718628\n",
      "20 200 losses: 1.305187702178955\n",
      "20 300 losses: 1.3366189002990723\n",
      "20 400 losses: 0.9761339426040649\n",
      "20 500 losses: 1.3123366832733154\n",
      "20 600 losses: 1.393886685371399\n",
      "20 700 losses: 1.2491557598114014\n",
      "20 800 losses: 1.2157542705535889\n",
      "20 900 losses: 0.969780683517456\n",
      "Epoch 21, Loss: 1.2290900945663452, Accuracy: 81.56613159179688, Test Loss: 2.8019015789031982, Test Accuracy: 41.530006408691406\n",
      "21 0 losses: 1.3586623668670654\n",
      "21 100 losses: 1.355879783630371\n",
      "21 200 losses: 1.1132663488388062\n",
      "21 300 losses: 1.0775694847106934\n",
      "21 400 losses: 1.060969352722168\n",
      "21 500 losses: 1.4300209283828735\n",
      "21 600 losses: 1.2421492338180542\n",
      "21 700 losses: 1.039871335029602\n",
      "21 800 losses: 1.326966404914856\n",
      "21 900 losses: 1.0162874460220337\n",
      "Epoch 22, Loss: 1.1989107131958008, Accuracy: 82.71208953857422, Test Loss: 2.760387659072876, Test Accuracy: 41.239994049072266\n",
      "22 0 losses: 1.1592180728912354\n",
      "22 100 losses: 1.3464428186416626\n",
      "22 200 losses: 1.1905195713043213\n",
      "22 300 losses: 1.30377995967865\n",
      "22 400 losses: 0.9682484269142151\n",
      "22 500 losses: 1.1319677829742432\n",
      "22 600 losses: 0.9268630743026733\n",
      "22 700 losses: 1.3001288175582886\n",
      "22 800 losses: 1.2863752841949463\n",
      "22 900 losses: 1.0476300716400146\n",
      "Epoch 23, Loss: 1.1711241006851196, Accuracy: 83.70223999023438, Test Loss: 2.735433340072632, Test Accuracy: 41.890010833740234\n",
      "23 0 losses: 1.2003790140151978\n",
      "23 100 losses: 1.2452306747436523\n",
      "23 200 losses: 1.1817535161972046\n",
      "23 300 losses: 1.2982388734817505\n",
      "23 400 losses: 1.1924494504928589\n",
      "23 500 losses: 1.0779513120651245\n",
      "23 600 losses: 1.0485972166061401\n",
      "23 700 losses: 1.2373794317245483\n",
      "23 800 losses: 1.1641851663589478\n",
      "23 900 losses: 1.104094386100769\n",
      "Epoch 24, Loss: 1.1433775424957275, Accuracy: 84.72215270996094, Test Loss: 2.781508684158325, Test Accuracy: 41.82001495361328\n",
      "24 0 losses: 1.1453386545181274\n",
      "24 100 losses: 1.2844375371932983\n",
      "24 200 losses: 1.3302465677261353\n",
      "24 300 losses: 1.5055878162384033\n",
      "24 400 losses: 1.1173179149627686\n",
      "24 500 losses: 1.183395504951477\n",
      "24 600 losses: 1.08185613155365\n",
      "24 700 losses: 1.2026145458221436\n",
      "24 800 losses: 1.224526286125183\n",
      "24 900 losses: 1.0798022747039795\n",
      "Epoch 25, Loss: 1.1189411878585815, Accuracy: 85.6441650390625, Test Loss: 2.674186944961548, Test Accuracy: 42.799991607666016\n",
      "25 0 losses: 1.130406379699707\n",
      "25 100 losses: 1.1543458700180054\n",
      "25 200 losses: 1.1609585285186768\n",
      "25 300 losses: 1.0803475379943848\n",
      "25 400 losses: 0.8863749504089355\n",
      "25 500 losses: 1.0904934406280518\n",
      "25 600 losses: 1.2501944303512573\n",
      "25 700 losses: 0.932009756565094\n",
      "25 800 losses: 1.117169976234436\n",
      "25 900 losses: 0.957756519317627\n",
      "Epoch 26, Loss: 1.0865325927734375, Accuracy: 86.85617065429688, Test Loss: 2.7312557697296143, Test Accuracy: 43.489994049072266\n",
      "26 0 losses: 0.8612057566642761\n",
      "26 100 losses: 0.8665607571601868\n",
      "26 200 losses: 1.1090595722198486\n",
      "26 300 losses: 1.1914842128753662\n",
      "26 400 losses: 1.0613197088241577\n",
      "26 500 losses: 1.0054184198379517\n",
      "26 600 losses: 0.9864510893821716\n",
      "26 700 losses: 1.1466199159622192\n",
      "26 800 losses: 1.0547693967819214\n",
      "26 900 losses: 0.9066880941390991\n",
      "Epoch 27, Loss: 1.0573264360427856, Accuracy: 88.02417755126953, Test Loss: 2.5895473957061768, Test Accuracy: 43.70000457763672\n",
      "27 0 losses: 1.1785588264465332\n",
      "27 100 losses: 1.2274737358093262\n",
      "27 200 losses: 1.1311184167861938\n",
      "27 300 losses: 1.0794224739074707\n",
      "27 400 losses: 0.9049347043037415\n",
      "27 500 losses: 1.0661344528198242\n",
      "27 600 losses: 1.1676011085510254\n",
      "27 700 losses: 1.0441131591796875\n",
      "27 800 losses: 1.037385106086731\n",
      "27 900 losses: 1.0342438220977783\n",
      "Epoch 28, Loss: 1.0379958152770996, Accuracy: 88.55818176269531, Test Loss: 2.633929967880249, Test Accuracy: 43.039981842041016\n",
      "28 0 losses: 0.9901513457298279\n",
      "28 100 losses: 1.206605076789856\n",
      "28 200 losses: 1.0467276573181152\n",
      "28 300 losses: 1.1637603044509888\n",
      "28 400 losses: 1.0872700214385986\n",
      "28 500 losses: 0.9646722674369812\n",
      "28 600 losses: 1.1706550121307373\n",
      "28 700 losses: 1.0248421430587769\n",
      "28 800 losses: 1.1837295293807983\n",
      "28 900 losses: 0.8797378540039062\n",
      "Epoch 29, Loss: 1.0267767906188965, Accuracy: 89.12017822265625, Test Loss: 2.736342668533325, Test Accuracy: 42.99998092651367\n",
      "29 0 losses: 1.19717538356781\n",
      "29 100 losses: 1.0594837665557861\n",
      "29 200 losses: 1.053063988685608\n",
      "29 300 losses: 1.0096930265426636\n",
      "29 400 losses: 1.0948646068572998\n",
      "29 500 losses: 1.0539450645446777\n",
      "29 600 losses: 0.8086430430412292\n",
      "29 700 losses: 1.2890368700027466\n",
      "29 800 losses: 1.0939817428588867\n",
      "29 900 losses: 0.922829270362854\n",
      "Epoch 30, Loss: 1.0027936697006226, Accuracy: 90.1662368774414, Test Loss: 2.6674611568450928, Test Accuracy: 43.510009765625\n",
      "30 0 losses: 1.0009963512420654\n",
      "30 100 losses: 1.0671619176864624\n",
      "30 200 losses: 1.189235806465149\n",
      "30 300 losses: 1.1923712491989136\n",
      "30 400 losses: 0.9726167321205139\n",
      "30 500 losses: 1.017356276512146\n",
      "30 600 losses: 0.9982070922851562\n",
      "30 700 losses: 1.0073246955871582\n",
      "30 800 losses: 0.9508845806121826\n",
      "30 900 losses: 0.888470470905304\n",
      "Epoch 31, Loss: 0.9923740029335022, Accuracy: 90.54019927978516, Test Loss: 2.6806716918945312, Test Accuracy: 42.38997268676758\n",
      "31 0 losses: 1.0462015867233276\n",
      "31 100 losses: 1.0525813102722168\n",
      "31 200 losses: 1.1080816984176636\n",
      "31 300 losses: 0.9845460653305054\n",
      "31 400 losses: 0.7401175498962402\n",
      "31 500 losses: 1.0460500717163086\n",
      "31 600 losses: 0.9772500395774841\n",
      "31 700 losses: 0.9394376873970032\n",
      "31 800 losses: 0.9423516988754272\n",
      "31 900 losses: 0.9954063296318054\n",
      "Epoch 32, Loss: 0.9762111902236938, Accuracy: 90.9461898803711, Test Loss: 2.6551740169525146, Test Accuracy: 43.39999008178711\n",
      "32 0 losses: 1.1367853879928589\n",
      "32 100 losses: 1.2430261373519897\n",
      "32 200 losses: 0.9431473612785339\n",
      "32 300 losses: 1.0938776731491089\n",
      "32 400 losses: 0.8461802005767822\n",
      "32 500 losses: 0.8311455249786377\n",
      "32 600 losses: 0.9258852601051331\n",
      "32 700 losses: 1.0222221612930298\n",
      "32 800 losses: 0.9834381341934204\n",
      "32 900 losses: 0.9103413224220276\n",
      "Epoch 33, Loss: 0.9548364281654358, Accuracy: 91.68215942382812, Test Loss: 2.681942939758301, Test Accuracy: 44.13001251220703\n",
      "33 0 losses: 0.8864568471908569\n",
      "33 100 losses: 0.9794507026672363\n",
      "33 200 losses: 1.0031806230545044\n",
      "33 300 losses: 0.8967654705047607\n",
      "33 400 losses: 0.8299560546875\n",
      "33 500 losses: 0.9107712507247925\n",
      "33 600 losses: 0.9003169536590576\n",
      "33 700 losses: 1.0323678255081177\n",
      "33 800 losses: 1.2568068504333496\n",
      "33 900 losses: 1.0303069353103638\n",
      "Epoch 34, Loss: 0.9460233449935913, Accuracy: 91.9981689453125, Test Loss: 2.7199056148529053, Test Accuracy: 43.18000411987305\n",
      "34 0 losses: 0.8855427503585815\n",
      "34 100 losses: 0.936413049697876\n",
      "34 200 losses: 0.7974093556404114\n",
      "34 300 losses: 0.8670654296875\n",
      "34 400 losses: 1.0993571281433105\n",
      "34 500 losses: 0.9067736268043518\n",
      "34 600 losses: 0.8092166185379028\n",
      "34 700 losses: 1.093788504600525\n",
      "34 800 losses: 0.8693168759346008\n",
      "34 900 losses: 0.8924758434295654\n",
      "Epoch 35, Loss: 0.9260603189468384, Accuracy: 92.67019653320312, Test Loss: 2.6767098903656006, Test Accuracy: 43.67000198364258\n",
      "35 0 losses: 0.9204627275466919\n",
      "35 100 losses: 1.0827254056930542\n",
      "35 200 losses: 0.9060680866241455\n",
      "35 300 losses: 0.9214962124824524\n",
      "35 400 losses: 0.935020923614502\n",
      "35 500 losses: 0.8916751146316528\n",
      "35 600 losses: 0.9799880981445312\n",
      "35 700 losses: 0.9256791472434998\n",
      "35 800 losses: 1.0436570644378662\n",
      "35 900 losses: 0.8664553165435791\n",
      "Epoch 36, Loss: 0.9239816665649414, Accuracy: 92.83827209472656, Test Loss: 2.6630945205688477, Test Accuracy: 44.2400016784668\n",
      "36 0 losses: 1.0096102952957153\n",
      "36 100 losses: 1.0079777240753174\n",
      "36 200 losses: 0.913575291633606\n",
      "36 300 losses: 0.7654640674591064\n",
      "36 400 losses: 0.9564083814620972\n",
      "36 500 losses: 0.9476024508476257\n",
      "36 600 losses: 0.8020272850990295\n",
      "36 700 losses: 0.8631685376167297\n",
      "36 800 losses: 0.8040899634361267\n",
      "36 900 losses: 0.8363478183746338\n",
      "Epoch 37, Loss: 0.9103151559829712, Accuracy: 93.4721908569336, Test Loss: 2.715637445449829, Test Accuracy: 43.31999588012695\n",
      "37 0 losses: 0.9428767561912537\n",
      "37 100 losses: 1.0428669452667236\n",
      "37 200 losses: 0.9166504740715027\n",
      "37 300 losses: 0.8366659283638\n",
      "37 400 losses: 0.8807138204574585\n",
      "37 500 losses: 1.031003713607788\n",
      "37 600 losses: 1.0267261266708374\n",
      "37 700 losses: 0.9223650097846985\n",
      "37 800 losses: 0.8800783753395081\n",
      "37 900 losses: 0.780810534954071\n",
      "Epoch 38, Loss: 0.9055820107460022, Accuracy: 93.64215850830078, Test Loss: 2.6941211223602295, Test Accuracy: 43.52000045776367\n",
      "38 0 losses: 0.90492844581604\n",
      "38 100 losses: 0.9142422676086426\n",
      "38 200 losses: 0.9674993753433228\n",
      "38 300 losses: 0.999187171459198\n",
      "38 400 losses: 0.8729007244110107\n",
      "38 500 losses: 0.7814236283302307\n",
      "38 600 losses: 0.9211524724960327\n",
      "38 700 losses: 0.7452225685119629\n",
      "38 800 losses: 0.9582071900367737\n",
      "38 900 losses: 0.9658342599868774\n",
      "Epoch 39, Loss: 0.8927399516105652, Accuracy: 93.9181900024414, Test Loss: 2.7379953861236572, Test Accuracy: 44.02000427246094\n",
      "39 0 losses: 1.0337218046188354\n",
      "39 100 losses: 1.0101313591003418\n",
      "39 200 losses: 1.0130504369735718\n",
      "39 300 losses: 1.0131884813308716\n",
      "39 400 losses: 0.9045383334159851\n",
      "39 500 losses: 0.8412243127822876\n",
      "39 600 losses: 0.9434301853179932\n",
      "39 700 losses: 0.9271509051322937\n",
      "39 800 losses: 0.7466743588447571\n",
      "39 900 losses: 0.8996729850769043\n",
      "Epoch 40, Loss: 0.8853505253791809, Accuracy: 94.20221710205078, Test Loss: 2.7391393184661865, Test Accuracy: 43.90998458862305\n",
      "40 0 losses: 0.8253520131111145\n",
      "40 100 losses: 0.9046338796615601\n",
      "40 200 losses: 0.8525443077087402\n",
      "40 300 losses: 0.7666866779327393\n",
      "40 400 losses: 0.9171366691589355\n",
      "40 500 losses: 0.9080957174301147\n",
      "40 600 losses: 0.8625892400741577\n",
      "40 700 losses: 0.972840428352356\n",
      "40 800 losses: 0.8693728446960449\n",
      "40 900 losses: 0.8951775431632996\n",
      "Epoch 41, Loss: 0.8804924488067627, Accuracy: 94.4461669921875, Test Loss: 2.8518218994140625, Test Accuracy: 44.369991302490234\n",
      "41 0 losses: 0.9629746079444885\n",
      "41 100 losses: 0.907871425151825\n",
      "41 200 losses: 0.9694848656654358\n",
      "41 300 losses: 0.9562941193580627\n",
      "41 400 losses: 0.8241751194000244\n",
      "41 500 losses: 1.001570463180542\n",
      "41 600 losses: 0.8884280323982239\n",
      "41 700 losses: 1.0345146656036377\n",
      "41 800 losses: 0.8089659810066223\n",
      "41 900 losses: 0.8300173878669739\n",
      "Epoch 42, Loss: 0.8729702234268188, Accuracy: 94.57815551757812, Test Loss: 2.835900068283081, Test Accuracy: 43.52998733520508\n",
      "42 0 losses: 0.9059980511665344\n",
      "42 100 losses: 0.9274579882621765\n",
      "42 200 losses: 0.8500595092773438\n",
      "42 300 losses: 0.8094151020050049\n",
      "42 400 losses: 0.8492873907089233\n",
      "42 500 losses: 0.8203033208847046\n",
      "42 600 losses: 0.7513961791992188\n",
      "42 700 losses: 0.7837768793106079\n",
      "42 800 losses: 0.835410475730896\n",
      "42 900 losses: 0.802584707736969\n",
      "Epoch 43, Loss: 0.8596107959747314, Accuracy: 95.03810119628906, Test Loss: 2.6504437923431396, Test Accuracy: 44.149993896484375\n",
      "43 0 losses: 1.0154333114624023\n",
      "43 100 losses: 0.9665607213973999\n",
      "43 200 losses: 0.9494563341140747\n",
      "43 300 losses: 0.8702765107154846\n",
      "43 400 losses: 0.7958062887191772\n",
      "43 500 losses: 0.8839040994644165\n",
      "43 600 losses: 0.8326980471611023\n",
      "43 700 losses: 0.8944236040115356\n",
      "43 800 losses: 0.8385367393493652\n",
      "43 900 losses: 0.7675042748451233\n",
      "Epoch 44, Loss: 0.8516054153442383, Accuracy: 95.35609436035156, Test Loss: 2.696742534637451, Test Accuracy: 44.54999923706055\n",
      "44 0 losses: 0.876076877117157\n",
      "44 100 losses: 1.0541107654571533\n",
      "44 200 losses: 1.0037769079208374\n",
      "44 300 losses: 0.9849269390106201\n",
      "44 400 losses: 0.765850841999054\n",
      "44 500 losses: 0.9527685642242432\n",
      "44 600 losses: 0.8029820322990417\n",
      "44 700 losses: 0.9324817061424255\n",
      "44 800 losses: 0.7300655841827393\n",
      "44 900 losses: 0.7331129312515259\n",
      "Epoch 45, Loss: 0.8479529619216919, Accuracy: 95.34215545654297, Test Loss: 2.810490131378174, Test Accuracy: 43.91999435424805\n",
      "45 0 losses: 0.8209075927734375\n",
      "45 100 losses: 0.9464930891990662\n",
      "45 200 losses: 0.9825689792633057\n",
      "45 300 losses: 0.87122642993927\n",
      "45 400 losses: 0.9648924469947815\n",
      "45 500 losses: 0.8871496319770813\n",
      "45 600 losses: 0.8316342234611511\n",
      "45 700 losses: 0.8398851156234741\n",
      "45 800 losses: 0.7761833071708679\n",
      "45 900 losses: 0.7075717449188232\n",
      "Epoch 46, Loss: 0.8445460200309753, Accuracy: 95.51811981201172, Test Loss: 2.7538046836853027, Test Accuracy: 45.01998519897461\n",
      "46 0 losses: 1.0928802490234375\n",
      "46 100 losses: 0.8667976260185242\n",
      "46 200 losses: 0.9236614108085632\n",
      "46 300 losses: 0.9686208963394165\n",
      "46 400 losses: 0.8216722011566162\n",
      "46 500 losses: 0.7645969986915588\n",
      "46 600 losses: 0.8227065801620483\n",
      "46 700 losses: 0.9105269908905029\n",
      "46 800 losses: 0.8571649789810181\n",
      "46 900 losses: 0.8774833083152771\n",
      "Epoch 47, Loss: 0.8364201784133911, Accuracy: 95.73812103271484, Test Loss: 2.6324737071990967, Test Accuracy: 45.56998825073242\n",
      "47 0 losses: 0.9245945811271667\n",
      "47 100 losses: 0.7977057099342346\n",
      "47 200 losses: 0.7900336980819702\n",
      "47 300 losses: 0.8538826704025269\n",
      "47 400 losses: 0.8455742001533508\n",
      "47 500 losses: 0.8138278126716614\n",
      "47 600 losses: 0.9152204990386963\n",
      "47 700 losses: 0.9124204516410828\n",
      "47 800 losses: 0.9114655256271362\n",
      "47 900 losses: 0.8482530117034912\n",
      "Epoch 48, Loss: 0.8299887180328369, Accuracy: 96.1220703125, Test Loss: 2.7003889083862305, Test Accuracy: 45.21998596191406\n",
      "48 0 losses: 0.8391649723052979\n",
      "48 100 losses: 0.8516668081283569\n",
      "48 200 losses: 0.7443708181381226\n",
      "48 300 losses: 0.8331729769706726\n",
      "48 400 losses: 0.7767491340637207\n",
      "48 500 losses: 0.9503997564315796\n",
      "48 600 losses: 0.8105334639549255\n",
      "48 700 losses: 0.9702361822128296\n",
      "48 800 losses: 0.7878150939941406\n",
      "48 900 losses: 0.9516769647598267\n",
      "Epoch 49, Loss: 0.8266243934631348, Accuracy: 96.04805755615234, Test Loss: 2.9127063751220703, Test Accuracy: 44.60000991821289\n",
      "49 0 losses: 0.9042456150054932\n",
      "49 100 losses: 0.8498851656913757\n",
      "49 200 losses: 0.8368974328041077\n",
      "49 300 losses: 0.959109902381897\n",
      "49 400 losses: 0.8338822722434998\n",
      "49 500 losses: 0.673594057559967\n",
      "49 600 losses: 0.713377833366394\n",
      "49 700 losses: 0.7621086239814758\n",
      "49 800 losses: 0.913311779499054\n",
      "49 900 losses: 0.7832328677177429\n",
      "Epoch 50, Loss: 0.8150407075881958, Accuracy: 96.47008514404297, Test Loss: 2.725494384765625, Test Accuracy: 45.27998733520508\n"
     ]
    }
   ],
   "source": [
    "model = resnet34()\n",
    "model.build(input_shape=(None,32,32,3))\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'_cutmix'\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "model.build(input_shape=(None,32,32,3))\n",
    "lr = 1e-4\n",
    "optimizer=optimizers.Adam(lr=lr)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "train_accuracy = tf.keras.metrics.Mean('train_accuracy', dtype=tf.float32)\n",
    "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
    "test_accuracy = tf.keras.metrics.Mean('test_accuracy', dtype=tf.float32)\n",
    "\n",
    "for epoch in range(50):\n",
    "    # if epoch % 5 == 4:\n",
    "    #     lr/=10\n",
    "    #     optimizer.lr = lr\n",
    "    for step,(x,y) in enumerate(train_db):\n",
    "        #这里做一个前向循环,将需要求解梯度放进来\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_onehot=tf.one_hot(y,depth=100)\n",
    "            x, y_onehot = cutmix_mask(x,y_onehot)\n",
    "            #[b,32,32,3] => [b,100]\n",
    "            logits=model(x)\n",
    "            #[b] => [b,100]\n",
    "            #compute loss\n",
    "            loss=tf.losses.categorical_crossentropy(y_onehot,logits)\n",
    "    \n",
    "            loss=tf.reduce_mean(loss)\n",
    "            \n",
    "        pred=tf.argmax(logits,axis=1)\n",
    "        pred=tf.cast(pred,dtype=tf.int32)\n",
    "        correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n",
    "        correct=tf.reduce_sum(correct)\n",
    "        acc = correct/x.shape[0]\n",
    "        \n",
    "        train_loss(loss)\n",
    "        train_accuracy(acc)\n",
    "        #计算gradient\n",
    "        grads=tape.gradient(loss,model.trainable_variables)\n",
    "        #传给优化器两个参数：grads和variable，完成梯度更新\n",
    "        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(epoch,step,'losses:',float(loss))\n",
    "            \n",
    "    \n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "        \n",
    "    total_num=0\n",
    "    total_correct=0\n",
    "    for x,y in test_db:\n",
    "        logits=model(x)\n",
    "        y_onehot = tf.one_hot(y,depth = 100)\n",
    "        loss=tf.losses.categorical_crossentropy(y_onehot,logits)\n",
    "        loss=tf.reduce_mean(loss)\n",
    "        \n",
    "        #prob=tf.nn.softmax(logits,axis=1)\n",
    "        pred=tf.argmax(logits,axis=1)\n",
    "        pred=tf.cast(pred,dtype=tf.int32)\n",
    "        correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n",
    "        correct=tf.reduce_sum(correct)\n",
    "\n",
    "        total_num += x.shape[0]\n",
    "        total_correct += int(correct)\n",
    "        \n",
    "        test_accuracy(correct/x.shape[0])\n",
    "        test_loss(loss)\n",
    "    \n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "            \n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                         train_loss.result(), \n",
    "                         train_accuracy.result()*100,\n",
    "                         test_loss.result(), \n",
    "                         test_accuracy.result()*100))\n",
    "\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "model.save_weights('cutmix.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
